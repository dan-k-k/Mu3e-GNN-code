{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader, Batch\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm, GINEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score, precision_recall_curve, roc_curve,\n",
    "    classification_report, auc, accuracy_score\n",
    ")\n",
    "from collections import Counter\n",
    "from sklearn.utils import resample\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Random Seed for Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # 'None' removes the limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN Training from the Graphs saved in the Graph generation notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the graphs saved in the graph generation notebook\n",
    "from loadgraphs import force_label_long, extract_unique_real, save_unique_real\n",
    "graphs = torch.load('Graphs_eee_May31.pt')\n",
    "test_graphs_IC = torch.load('Graphs_eeevv_May31.pt')\n",
    "test_graphs_M = torch.load('Graphs_evv_May31.pt')\n",
    "test_graphs_beam = torch.load('Graphs_beam_May31.pt')\n",
    "\n",
    "graphs = force_label_long(graphs)\n",
    "test_graphs_IC = force_label_long(test_graphs_IC)\n",
    "test_graphs_M = force_label_long(test_graphs_M)\n",
    "test_graphs_beam = force_label_long(test_graphs_beam)\n",
    "\n",
    "# eee signal split into training, validation and testing sets\n",
    "train_graphs, test_graphs = train_test_split(graphs, test_size=0.2, random_state=43)\n",
    "train_graphs, val_graphs = train_test_split(train_graphs, test_size=0.2, random_state=43)\n",
    "print('signal: train',len(train_graphs),'val',len(val_graphs),'test',len(test_graphs))\n",
    "train_loader = DataLoader(train_graphs, batch_size=128, shuffle=True, exclude_keys=['mc_pid', 'mc_tid'])\n",
    "val_loader = DataLoader(val_graphs, batch_size=128, exclude_keys=['mc_pid', 'mc_tid'])\n",
    "test_loader = DataLoader(test_graphs, batch_size=128, exclude_keys=['mc_pid', 'mc_tid'])\n",
    "\n",
    "label_counts = Counter([data.label.item() for data in graphs])\n",
    "print(\"Graph counts per class (signal):\", label_counts)\n",
    "\n",
    "# eeevv internal conversion\n",
    "test_loader_IC = DataLoader(test_graphs_IC, batch_size=128, shuffle=False, exclude_keys=['mc_pid'])\n",
    "\n",
    "label_counts = Counter([data.label.item() for data in test_graphs_IC])\n",
    "print(\"Graph counts per class (IC):\", label_counts)\n",
    "\n",
    "# evv Michel\n",
    "test_loader_M = DataLoader(test_graphs_M, batch_size=128, shuffle=False, exclude_keys=['mc_pid'])\n",
    "\n",
    "print(f\"Loaded {len(test_graphs_M)} Michel (evv) graphs for testing.\")\n",
    "label_counts = Counter([data.label.item() for data in test_graphs_M])\n",
    "print(\"Graph counts per class (Michel):\", label_counts)\n",
    "\n",
    "# beam\n",
    "test_loader_beam = DataLoader(test_graphs_beam, batch_size=128, shuffle=False, exclude_keys=['mc_pid'])\n",
    "\n",
    "print(f\"Loaded {len(test_graphs_beam)} beam graphs for testing.\")\n",
    "label_counts = Counter([data.label.item() for data in test_graphs_beam])\n",
    "print(\"Graph counts per class (beam):\", label_counts)\n",
    "\n",
    "# save the mc_tids (and frameids) as this lets you know directly of the constraint/graph generation \n",
    "# efficiency before GNN efficiency is found.\n",
    "# below finds only for real graphs (with labels 0 and 1).\n",
    "save_unique_real(graphs,            \"postconstraints_ids_signal.csv\")\n",
    "save_unique_real(test_graphs,       \"testsubset_ids_signal.csv\")\n",
    "save_unique_real(test_graphs_IC,    \"postconstraints_ids_IC.csv\")\n",
    "save_unique_real(test_graphs_M,     \"postconstraints_ids_michel.csv\")\n",
    "save_unique_real(test_graphs_beam,  \"postconstraints_ids_beam.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train (GCN or GINE)\n",
    "from defineGNNmodel import GCNMultiClass, GINEMultiClass\n",
    "# Purity is the number of correct predictions out of all predictions made for a specific class (ie class e+, e- or fake).\n",
    "# Efficiency is the number of correct predictions out of all available graphs in a class.\n",
    "# the report has results for the GCN with 31May graphs as they are setup: in_channels=11, hidden_channels=64, extra_features=10\n",
    "# i have also tested a GINE model and it produces very similar results.\n",
    "# models need to be trained once and can be reloaded. make sure to save the model in the next cell.\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # \n",
    "\n",
    "model = GCNMultiClass(in_channels=11, hidden_channels=64, extra_features=10).to(device) \n",
    "# model = GINEMultiClass(in_channels=11, hidden_channels=64, edge_channels=5, extra_features=10).to(device) \n",
    "\n",
    "# Compute pos_weight based on training set\n",
    "num_p = sum(1 for data in train_graphs if data.label.item() == 0)  # positron\n",
    "num_e = sum(1 for data in train_graphs if data.label.item() == 1)  # electron\n",
    "num_fake = sum(1 for data in train_graphs if data.label.item() == 2) # fake\n",
    "print(\"Positron count:\", num_p, \"Electron count:\", num_e, \"Fake count:\", num_fake)\n",
    "\n",
    "class_counts = np.array([num_p, num_e, num_fake], dtype=np.float32)\n",
    "total_samples = class_counts.sum()\n",
    "num_classes = 3\n",
    "class_weights = total_samples / (num_classes * class_counts)\n",
    "print(\"Class weights [e+, e-, fake]:\", class_weights)\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor) # it performs better with no weighting applied\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training parameters logits best-class predictions\n",
    "epochs = 100\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "best_model_state = copy.deepcopy(model.state_dict())\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    all_train_preds = []\n",
    "    all_train_labels = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch)  # outputs shape: [batch_size, 3]\n",
    "        loss = criterion(outputs, batch.label)  # batch.label is [batch_size] of integers\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * batch.num_graphs\n",
    "        \n",
    "        # Predictions: take the argmax of logits\n",
    "        preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "        labels = batch.label.detach().cpu().numpy()\n",
    "        all_train_preds.extend(preds)\n",
    "        all_train_labels.extend(labels)\n",
    "    \n",
    "    avg_train_loss = epoch_loss / len(train_loader.dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch)\n",
    "            loss = criterion(outputs, batch.label)\n",
    "            val_loss += loss.item() * batch.num_graphs\n",
    "            preds = torch.argmax(outputs, dim=1).detach().cpu().numpy()\n",
    "            labels = batch.label.detach().cpu().numpy()\n",
    "            val_preds.extend(preds)\n",
    "            val_labels.extend(labels)\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    \n",
    "    train_acc = accuracy_score(all_train_labels, all_train_preds)\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    precision_per_class = precision_score(val_labels, val_preds, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(val_labels, val_preds, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(val_labels, val_preds, average=None, zero_division=0)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    print(\"Purity per class:\", precision_per_class)\n",
    "    print(\"Efficiency per class:\", recall_per_class)\n",
    "    print(\"F1 per class:\", f1_per_class)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model_state = copy.deepcopy(model.state_dict())\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Load best model state\n",
    "model.load_state_dict(best_model_state)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'TrainedGCNModelJun02.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from defineGNNmodel import GCNMultiClass, GINEMultiClass\n",
    "model = GCNMultiClass(in_channels=11, hidden_channels=64, extra_features=10).to(device) # \n",
    "model.load_state_dict(torch.load('TrainedGCNModelJun02.pt', map_location=device))\n",
    "\n",
    "# model = GINEMultiClass(in_channels=11, hidden_channels=128, edge_channels=5, extra_features=10).to(device) \n",
    "# model.load_state_dict(torch.load('TrainedGINEModelJun02.pt', map_location=device))\n",
    "\n",
    "# Set the model to evaluation mode.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"first graph\", graphs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Train and Validation Loss vs Epoch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "plt.plot(epochs_range, train_losses, label=\"Training Loss\")\n",
    "plt.plot(epochs_range, val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\", fontsize=12)\n",
    "plt.ylabel(\"Loss\", fontsize=12)\n",
    "plt.tick_params(axis=\"both\", labelsize=11)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "best_loss = min(val_losses)\n",
    "best_epoch = np.argmin(val_losses) + 1  # +1 to convert from 0-index to epoch number\n",
    "print(f\"Best Validation Loss: {best_loss:.4f} at Epoch: {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEDUPLICATE the real graphs (not overlap removal)\n",
    "# this cell ensures an mc_tid appears only once in all real (0 for e+, 1 for e-) graphs.\n",
    "# graphs have duplicates in the sense that the double hit has led to two graphs that only differ by one hit. as the double\n",
    "# hits both lie on the particle's bending circle, either graph can be saved. here, i save the one with higher GNN output.\n",
    "# this requires mc_tid, so this is done only for analysis but if this were to run on real beam data, a method to \n",
    "# determine if duplicate graphs belong to the same particle is needed.\n",
    "# this is only done for real graphs for correct efficiency calculation.\n",
    "from deduplicategraphs import deduplicate_real_graphs, compute_real_graph_preds\n",
    "\n",
    "# find preds of real graphs for deduplication\n",
    "compute_real_graph_preds(test_graphs, model, device)\n",
    "compute_real_graph_preds(test_graphs_beam, model, device)\n",
    "compute_real_graph_preds(test_graphs_M, model, device)\n",
    "compute_real_graph_preds(test_graphs_IC, model, device)\n",
    "\n",
    "# Apply deduplication to the test sets.\n",
    "d_test_graphs = deduplicate_real_graphs(test_graphs)\n",
    "d_test_graphs_beam = deduplicate_real_graphs(test_graphs_beam)\n",
    "d_test_graphs_M = deduplicate_real_graphs(test_graphs_M)\n",
    "d_test_graphs_IC = deduplicate_real_graphs(test_graphs_IC)\n",
    "print(\"Total test graphs before,after deduplication (eee):\", len(test_graphs),',', len(d_test_graphs))\n",
    "print(\"Total test graphs before,after deduplication (beam):\", len(test_graphs_beam),',', len(d_test_graphs_beam))\n",
    "print(\"Total test graphs before,after deduplication (M):\", len(test_graphs_M),',', len(d_test_graphs_M))\n",
    "print(\"Total test graphs before,after deduplication (IC):\", len(test_graphs_IC),',', len(d_test_graphs_IC))\n",
    "\n",
    "# Create a new DataLoader from the deduplicated test graphs\n",
    "# remove keys that only appear in real graphs\n",
    "d_test_loader = DataLoader(d_test_graphs, batch_size=128,           exclude_keys=['mc_pid', 'mc_tid', 'pred_confidence', 'pred_label', 'removal_counts'])\n",
    "d_test_loader_beam = DataLoader(d_test_graphs_beam, batch_size=128, exclude_keys=['mc_pid', 'mc_tid', 'pred_confidence', 'pred_label', 'removal_counts'])\n",
    "d_test_loader_M = DataLoader(d_test_graphs_M, batch_size=128,       exclude_keys=['mc_pid', 'mc_tid', 'pred_confidence', 'pred_label', 'removal_counts'])\n",
    "d_test_loader_IC = DataLoader(d_test_graphs_IC, batch_size=128,     exclude_keys=['mc_pid', 'mc_tid', 'pred_confidence', 'pred_label', 'removal_counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pre-OR efficiencies with clopper pearson 1sigma errors\n",
    "# efficiency is the number of real graphs predicted real out of all real graphs available\n",
    "# purity     is the number of real graphs predicted real out of all graphs predicted real\n",
    "# the terms precision (for purity) and recall (for efficiency) are used by the default library classification_report\n",
    "from evaluategraphs import evaluate_combined, evaluate_for_class, evaluate_with_argmax_combined, evaluate_class_with_argmax, efficiency_with_CP\n",
    "\n",
    "y_true_sig,  y_pred_sig, cm, cr  = evaluate_with_argmax_combined(model, d_test_loader,    device)\n",
    "y_true_beam, y_pred_beam, _, _ = evaluate_with_argmax_combined(model, d_test_loader_beam, device)\n",
    "y_true_m,    y_pred_m, _, _    = evaluate_with_argmax_combined(model, d_test_loader_M,    device)\n",
    "y_true_ic,   y_pred_ic, _, _   = evaluate_with_argmax_combined(model, d_test_loader_IC,   device)\n",
    "print(\"signal: confusion matrix and classification report\")\n",
    "print(cm)\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clopper pearson one sigma errors before O.R\n",
    "pre_metrics_eee  = efficiency_with_CP(y_true_sig,  y_pred_sig,  \"Signal\")\n",
    "pre_metrics_beam = efficiency_with_CP(y_true_beam, y_pred_beam, \"Beam\")\n",
    "pre_metrics_evv  = efficiency_with_CP(y_true_m,    y_pred_m,    \"Michel\")\n",
    "pre_metrics_eeevv = efficiency_with_CP(y_true_ic,   y_pred_ic,   \"I.C.\")\n",
    "\n",
    "all_pre_metrics = {\n",
    "    'Beam': pre_metrics_beam,\n",
    "    'Signal': pre_metrics_eee,\n",
    "    'I.C.': pre_metrics_eeevv,\n",
    "    'Michel': pre_metrics_evv\n",
    "}\n",
    "\n",
    "# For plotting, extract efficiencies and confidence intervals:\n",
    "datasets = []\n",
    "efficiencies = []\n",
    "lower_bounds = []\n",
    "upper_bounds = []\n",
    "\n",
    "for ds, met in all_pre_metrics.items():\n",
    "    if met:  # skip empty dictionaries\n",
    "        datasets.append(ds)\n",
    "        efficiencies.append(met['efficiency'])\n",
    "        lower_bounds.append(met['lower_bound'])\n",
    "        upper_bounds.append(met['upper_bound'])\n",
    "\n",
    "# Compute symmetric error bars:\n",
    "errors = [(up - low) / 2 for up, low in zip(upper_bounds, lower_bounds)]\n",
    "\n",
    "# Now you can use these values for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_pos = np.arange(len(datasets))\n",
    "\n",
    "plt.figure(figsize=(6.3, 1.33))\n",
    "plt.errorbar(efficiencies, y_pos, xerr=errors, fmt='o', color='black', ecolor='gray', capsize=5, label='Clopper–Pearson\\n1σ errors')\n",
    "plt.yticks(y_pos, datasets, fontsize=12)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.xlabel(\"Pre-O.R. Efficiency\", fontsize=14)\n",
    "plt.legend(fontsize=11, framealpha=0.7, bbox_to_anchor=(1.4,1))\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xlim(0.993, 1.00)  # adjust as needed\n",
    "plt.ylim(-0.5, len(datasets) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find (pre OR) per-class results for signal the plot in the cell below\n",
    "results_electrons = evaluate_for_class(model, d_test_loader, class_index=1, dataset_name=\"Test Set - Electrons\")\n",
    "results_positrons = evaluate_for_class(model, d_test_loader, class_index=0, dataset_name=\"Test Set - Positrons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi; eee; combine electron and positron curves. \n",
    "print(\"eee\")\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.plot(results_electrons['efficiency'], results_electrons['brr'], \n",
    "         label='Electrons', color='blue', lw=2)\n",
    "plt.plot(results_positrons['efficiency'], results_positrons['brr'], \n",
    "         label='Positrons', color='red', lw=2)\n",
    "plt.axhline(y=1, color='gray', linestyle='--', lw=2, label='No Discriminatory Rejection')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Efficiency', fontsize=14)\n",
    "plt.ylabel('Background Rejection Rate', fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.ylim(top=1e5)\n",
    "plt.xlim(-0.05, 1.05)\n",
    "plt.legend(loc=\"lower left\", fontsize=11, framealpha=0.3)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# For electrons:\n",
    "fpr_e, tpr_e, _ = roc_curve(results_electrons['y_true'], results_electrons['y_probs'])\n",
    "roc_auc_e = auc(fpr_e, tpr_e)\n",
    "# For positrons:\n",
    "fpr_p, tpr_p, _ = roc_curve(results_positrons['y_true'], results_positrons['y_probs'])\n",
    "roc_auc_p = auc(fpr_p, tpr_p)\n",
    "epsilon = 1e-6\n",
    "# fpr_nonzero = np.maximum(fpr, epsilon)\n",
    "fpr_diag = np.linspace(epsilon, 1, 100)\n",
    "\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.plot(fpr_e, tpr_e, color='blue', lw=2, \n",
    "         label=f'Electrons AUC = {roc_auc_e:.4f}')\n",
    "plt.plot(fpr_p, tpr_p, color='red', lw=2, \n",
    "         label=f'Positrons AUC = {roc_auc_p:.4f}')\n",
    "# plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.plot(fpr_diag, np.linspace(0, 1, 100), color='grey', linestyle='--', label='')\n",
    "plt.xscale('log', base=10)\n",
    "plt.xlim(5e-5, 1)\n",
    "plt.xlabel('False Positive Rate', fontsize=14)\n",
    "plt.ylabel('Efficiency', fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.legend(loc=\"lower right\", fontsize=11, framealpha=0.6)#, bbox_to_anchor=(0.5, 1.3))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e+ and e- separation confusion matrix (pre O.R.)\n",
    "from evaluategraphs import e_separation\n",
    "\n",
    "e_separation(d_test_graphs, \"eee\", model, device)\n",
    "e_separation(d_test_graphs_beam, \"Beam\", model, device)\n",
    "e_separation(d_test_graphs_M, \"evv\", model, device)\n",
    "e_separation(d_test_graphs_IC, \"eeevv\", model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply overlap removal to the test sets\n",
    "from overlapremoval import perform_overlap_removal\n",
    "\n",
    "OR_test_graphs, removal_log, removal_confidence_diffs, removed_true_real, removed_true_fake = perform_overlap_removal(d_test_graphs, model, dataset_name=\"eee\")\n",
    "OR_test_graphs_beam, removal_log_beam, removal_confidence_diffs_beam, removed_true_real_beam, removed_true_fake_beam = perform_overlap_removal(d_test_graphs_beam, model, dataset_name=\"Beam\")\n",
    "OR_test_graphs_M, removal_log_M, removal_confidence_diffs_M, removed_true_real_M, removed_true_fake_M = perform_overlap_removal(d_test_graphs_M, model, dataset_name=\"evv\")\n",
    "OR_test_graphs_IC, removal_log_IC, removal_confidence_diffs_IC, removed_true_real_IC, removed_true_fake_IC = perform_overlap_removal(d_test_graphs_IC, model, dataset_name=\"eeevv\")\n",
    "\n",
    "print(\"Overlap Removal Log (eee):\", removal_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the mc_tids and frameids of the tracks that have now survived the full process:\n",
    "# unique hits > constraints > overlap removal > GNN output above 0.5 on surivors\n",
    "from evaluategraphs import save_OR_mc_tids\n",
    "save_OR_mc_tids(OR_test_graphs, \"postOR_ids_signal.csv\", confidence_threshold=0.5)\n",
    "# save_OR_mc_tids(OR_test_graphs_IC, \"postOR_ids_signal_IC.csv\", confidence_threshold=0.5)\n",
    "# save_OR_mc_tids(OR_test_graphs_M, \"postOR_ids_signal_michel.csv\", confidence_threshold=0.5)\n",
    "# save_OR_mc_tids(OR_test_graphs_beam, \"postOR_ids_signal_beam.csv\", confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view results after OR (clopper pearson 1sigma errors)\n",
    "from evaluategraphs import compute_OR_metrics\n",
    "\n",
    "post_metrics_eee = compute_OR_metrics(d_test_graphs, OR_test_graphs, \"eee\")\n",
    "post_metrics_evv = compute_OR_metrics(d_test_graphs_M, OR_test_graphs_M, \"evv\")\n",
    "post_metrics_eeevv = compute_OR_metrics(d_test_graphs_IC, OR_test_graphs_IC, \"eeevv\")\n",
    "post_metrics_beam = compute_OR_metrics(d_test_graphs_beam, OR_test_graphs_beam, \"Beam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clopper pearson one sigma errors after O.R.\n",
    "all_metrics = {\n",
    "    'Beam': {}, # post_metrics_beam \n",
    "    'Michel': post_metrics_evv,\n",
    "    'I.C.': post_metrics_eeevv,\n",
    "    'Signal': post_metrics_eee\n",
    "}\n",
    "\n",
    "# For plotting, extract efficiencies and confidence intervals:\n",
    "datasets = []\n",
    "efficiencies = []\n",
    "lower_bounds = []\n",
    "upper_bounds = []\n",
    "\n",
    "for ds, met in all_metrics.items():\n",
    "    if met:  # skip empty dictionaries\n",
    "        datasets.append(ds)\n",
    "        efficiencies.append(met['final_efficiency'])\n",
    "        lower_bounds.append(met['lower_bound'])\n",
    "        upper_bounds.append(met['upper_bound'])\n",
    "\n",
    "# Compute symmetric error bars:\n",
    "errors = [(up - low) / 2 for up, low in zip(upper_bounds, lower_bounds)]\n",
    "\n",
    "# Now you can use these values for plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "y_pos = np.arange(len(datasets))\n",
    "\n",
    "plt.figure(figsize=(6.3, 1.0))\n",
    "plt.errorbar(efficiencies, y_pos, xerr=errors, fmt='o', color='black', ecolor='gray', capsize=5, label='Clopper–Pearson\\n1σ errors')\n",
    "plt.yticks(y_pos, datasets, fontsize=12)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.xlabel(\"Post-O.R. Efficiency\", fontsize=14)\n",
    "plt.legend(fontsize=11, framealpha=0.7, bbox_to_anchor=(1.4,1))\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xlim(0.993, 1.00)  # adjust as needed\n",
    "plt.ylim(-0.5, len(datasets) - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view tracks that were true real, predicted real but then removed by a true fake with higher confidence with a shared hit\n",
    "for g in removed_true_real[:10]:    \n",
    "    print(f\"Frame {g.frameId}, tid {g.mc_tid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly visualisation to look at frames in 'removed_true_real'\n",
    "from plotlyhelper import visualise_truth_tracks, wireframe_traces\n",
    "tracks_df = torch.load('true_tracks_eee_May31.pt')\n",
    "target_frame_id = 22205  # Replace with desired frame id\n",
    "visualise_truth_tracks(target_frame_id, tracks_df, wireframe_traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create DFs for all sets\n",
    "# allows you to see histograms related to efficiency, purity and graph features or truth information\n",
    "from create_dfs import create_df_from_graphs\n",
    "\n",
    "df_eee    = create_df_from_graphs(d_test_graphs, model)\n",
    "df_IC     = create_df_from_graphs(d_test_graphs_IC, model)\n",
    "df_michel = create_df_from_graphs(d_test_graphs_M, model)\n",
    "df_beam   = create_df_from_graphs(d_test_graphs_beam, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chord lengths and purity\n",
    "# split df_eee into its classes to do a plot like below\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "df_positrons = df_eee[df_eee['label'] == 0]   # actually only needed if you want to inspect true counts\n",
    "df_electrons  = df_eee[df_eee['label'] == 1]\n",
    "df_fake       = df_eee[df_eee['label'] == 2]\n",
    "\n",
    "# ----- Setup for Chord Length -----\n",
    "n_bins = 20\n",
    "min_val = df_eee['chord_length'].min()\n",
    "max_val = df_eee['chord_length'].max()\n",
    "bins = np.linspace(min_val, max_val, n_bins + 1)\n",
    "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "bar_width = bins[1] - bins[0]\n",
    "\n",
    "# ----- Figure 1: Total Counts for Chord Length -----\n",
    "# Compute total counts per bin for each class.\n",
    "total_e, _ = np.histogram(df_electrons['chord_length'], bins=bins)\n",
    "total_p, _ = np.histogram(df_positrons['chord_length'], bins=bins)\n",
    "total_f, _ = np.histogram(df_fake['chord_length'], bins=bins)\n",
    "\n",
    "plt.figure(figsize=(4.5,3))\n",
    "# Plot electrons as blue unfilled circles.\n",
    "plt.scatter(bin_centers, total_e, marker='o', s=50, linewidth=1.5, \n",
    "            facecolors='none', edgecolors='blue', label='Total Electrons')\n",
    "# Plot positrons as red plus markers.\n",
    "plt.scatter(bin_centers, total_p, marker='+', s=75, color='red', label='Total Positrons')\n",
    "# Plot fakes as an unfilled histogram.\n",
    "plt.hist(df_fake['chord_length'], bins=bins, histtype='step', \n",
    "         color='grey', linewidth=2, label='Total Fakes')\n",
    "\n",
    "plt.xlabel(\"Chord Length (mm)\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.2,1))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# ----- Figure 2: Purity and Efficiency vs. Chord Length -----\n",
    "def compute_pred_counts_and_purity(df, bins, column, label):\n",
    "    # 1) total_pred_label: how many were predicted as class `label` in each bin?\n",
    "    mask_pred = (df['pred_label'] == label)\n",
    "    total_pred_label, _ = np.histogram(df.loc[mask_pred, column], bins=bins)\n",
    "\n",
    "    # 2) correct_pred_label: of those predicted==label, how many were truly label==label?\n",
    "    mask_correct = (df['pred_label'] == label) & (df['label'] == label)\n",
    "    correct_pred_label, _ = np.histogram(df.loc[mask_correct, column], bins=bins)\n",
    "\n",
    "    # 3) purity_label = correct_pred_label / total_pred_label  (NaN if total_pred_label == 0)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        purity_label = correct_pred_label / total_pred_label\n",
    "        purity_label[total_pred_label == 0] = np.nan\n",
    "\n",
    "    # 4) total_true_label: how many truly belong to `label` in each bin?\n",
    "    mask_true = (df['label'] == label)\n",
    "    total_true_label, _ = np.histogram(df.loc[mask_true, column], bins=bins)\n",
    "\n",
    "    # 5) efficiency_label = correct_pred_label / total_true_label  (NaN if total_true_label == 0)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        efficiency_label = correct_pred_label / total_true_label\n",
    "        efficiency_label[total_true_label == 0] = np.nan\n",
    "\n",
    "    return total_pred_label, correct_pred_label, purity_label, total_true_label, efficiency_label\n",
    "\n",
    "tot_pred_e, corr_pred_e, purity_e, tot_true_e, eff_e = compute_pred_counts_and_purity(df_eee, bins, 'path_length', label=1)\n",
    "tot_pred_p, corr_pred_p, purity_p, tot_true_p, eff_p = compute_pred_counts_and_purity(df_eee, bins, 'path_length', label=0)\n",
    "tot_pred_f, corr_pred_f, purity_f, tot_true_f, eff_f = compute_pred_counts_and_purity(df_eee, bins, 'path_length', label=2)\n",
    "\n",
    "# Now you can plot Purity vs. feature:\n",
    "plt.figure(figsize=(4.5,3))\n",
    "plt.scatter(bin_centers, purity_e, marker='o', edgecolor='blue', facecolors='none', label='Electron Purity')\n",
    "plt.scatter(bin_centers, purity_p, marker='+', color='red',   label='Positron Purity')\n",
    "plt.step(bin_centers,     purity_f, where='mid', color='gray',linewidth=2, label='Fake Purity')\n",
    "\n",
    "plt.xlabel(\"Path Length (mm)\", fontsize=14)\n",
    "plt.ylabel(\"Purity (Precision)\", fontsize=14)\n",
    "plt.ylim(0,1.05)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# And separately plot Efficiency vs. feature:\n",
    "plt.figure(figsize=(4.5,3))\n",
    "plt.scatter(bin_centers, eff_e, marker='o', edgecolor='blue', facecolors='none', label='Electron Efficiency')\n",
    "plt.scatter(bin_centers, eff_p, marker='+', color='red',   label='Positron Efficiency')\n",
    "plt.step(bin_centers,     eff_f, where='mid', color='gray',linewidth=2, label='Fake Efficiency')\n",
    "\n",
    "plt.xlabel(\"Path Length (mm)\", fontsize=14)\n",
    "plt.ylabel(\"Efficiency (Recall)\", fontsize=14)\n",
    "plt.ylim(0,1.05)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficiency against truth info!\n",
    "from GNNplots import plot_efficiency_vs_feature_step\n",
    "# 1) true momentum mc_p\n",
    "plot_efficiency_vs_feature_step(df_signal=df_eee, df_IC=df_IC, df_michel=df_michel, df_beam=df_beam, feature=\"mc_p\", x_label=r\"$p_{\\mathrm{true}}$ [MeV]\", label_signal=\"Signal\", label_IC=\"I.C.\", label_michel=\"Michel\", label_beam=\"Beam\", bins=40)\n",
    "# 2) true transverse momentum mc_pt\n",
    "plot_efficiency_vs_feature_step(df_signal=df_eee, df_IC=df_IC, df_michel=df_michel, df_beam=df_beam, feature=\"mc_pt\", x_label=r\"$p_{T,\\mathrm{true}}$ [MeV]\", label_signal=\"Signal\", label_IC=\"I.C.\", label_michel=\"Michel\", label_beam=\"Beam\", bins=40)\n",
    "# 3) true phi mc_phi\n",
    "plot_efficiency_vs_feature_step(df_signal=df_eee, df_IC=df_IC, df_michel=df_michel, df_beam=df_beam, feature=\"mc_phi\", x_label=r\"$\\phi_{\\mathrm{true}}$ [rad]\", label_signal=\"Signal\", label_IC=\"I.C.\", label_michel=\"Michel\", label_beam=\"Beam\", bins=40)\n",
    "# 4) true lambda mc_lam\n",
    "plot_efficiency_vs_feature_step(df_signal=df_eee, df_IC=df_IC, df_michel=df_michel, df_beam=df_beam, feature=\"mc_lam\", x_label=r\"$\\lambda_{\\mathrm{true}}$ [rad]\", label_signal=\"Signal\", label_IC=\"I.C.\", label_michel=\"Michel\", label_beam=\"Beam\", bins=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average purity vs n graphs in a frame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_purity_vs_graphcount_multi(\n",
    "    dfs, labels, colors, linestyles,\n",
    "    max_graphs=800, bin_width=40\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlayed step‐plots of average purity vs. #graphs/frame,\n",
    "    grouping counts into bins of width `bin_width`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(4.5,3))\n",
    "    \n",
    "    # 1) Define the half‐shifted edges for the step plot:\n",
    "    edges = np.arange(0.5, max_graphs + bin_width + 0.5, bin_width)\n",
    "    # 2) Compute the bin‐centres for plotting & ticks\n",
    "    centers = edges[:-1] + bin_width/2\n",
    "\n",
    "    for df, label, c, ls in zip(dfs, labels, colors, linestyles):\n",
    "        by_frame = (\n",
    "            df.groupby('frameId')\n",
    "              .agg(TP=('TP','sum'),\n",
    "                   FP=('FP','sum'),\n",
    "                   n_graphs=('frameId','size'))\n",
    "              .reset_index()\n",
    "        )\n",
    "        by_frame['purity'] = by_frame['TP'] / (by_frame['TP'] + by_frame['FP'])\n",
    "        by_frame = by_frame.dropna(subset=['purity'])\n",
    "\n",
    "        # cap counts > max_graphs\n",
    "        by_frame.loc[by_frame['n_graphs'] > max_graphs, 'n_graphs'] = max_graphs\n",
    "\n",
    "        # assign to bin index\n",
    "        bin_idx = np.floor((by_frame['n_graphs'] - 1) / bin_width).astype(int)\n",
    "        by_frame['bin'] = bin_idx\n",
    "\n",
    "        # average purity per bin\n",
    "        summary = (\n",
    "            by_frame.groupby('bin')['purity']\n",
    "                    .mean()\n",
    "                    .reindex(range(len(centers)), fill_value=np.nan)\n",
    "        )\n",
    "\n",
    "        # build step coords\n",
    "        xs = np.repeat(edges, 2)[1:-1]\n",
    "        ys = np.repeat(summary.values, 2)\n",
    "\n",
    "        plt.step(xs, ys, where='pre', color=c, linestyle=ls, label=label)\n",
    "\n",
    "    plt.xlabel(\"Number of Graphs per Frame\", fontsize=14)\n",
    "    plt.ylabel(\"Average Purity\",              fontsize=14)\n",
    "    plt.tick_params(axis='both', labelsize=12)\n",
    "    plt.xlim(0.5, max_graphs + 0.5)\n",
    "\n",
    "    # only label every other bin centre\n",
    "    tick_centres = centers[::2]\n",
    "    tick_labels  = [f\"{int(c)}\" for c in tick_centres]\n",
    "    plt.xticks(tick_centres, tick_labels, rotation=45)\n",
    "\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    # plt.legend(fontsize=11, loc=\"lower right\", bbox_to_anchor=(1.33,0))\n",
    "    plt.show()\n",
    "\n",
    "# Usage\n",
    "dfs        = [df_eee, df_IC, df_michel, df_beam]\n",
    "labels     = [\"Signal\", \"I.C.\", \"Michel\", \"Beam\"]\n",
    "colors     = [\"blue\",  \"orange\",    \"red\",    \"green\"]\n",
    "linestyles = [\"-\",     \"--\",         \":\",      \"-.\"]\n",
    "\n",
    "plot_purity_vs_graphcount_multi(\n",
    "    dfs, labels, colors, linestyles,\n",
    "    max_graphs=800,\n",
    "    bin_width=40   # combine every 100 graph‐counts into one bin\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average purity vs n hits and n graphs in a frame \n",
    "from GNNplots import plot_purity_vs_graphcount_multi, plot_purity_vs_hitcount_multi\n",
    "dfs        = [df_eee, df_IC, df_michel, df_beam]\n",
    "labels     = [\"Signal\", \"I.C.\", \"Michel\", \"Beam\"]\n",
    "colors     = [\"blue\",  \"orange\",    \"red\",    \"green\"]\n",
    "linestyles = [\"-\",     \"--\",         \":\",      \"-.\"]\n",
    "\n",
    "plot_purity_vs_graphcount_multi(dfs, labels, colors, linestyles, max_graphs=800, bin_width=40)\n",
    "\n",
    "hits_df_eee   = torch.load('unique_hits_eee.pt')\n",
    "hits_df_eeevv = torch.load('unique_hits_eeevv.pt')\n",
    "hits_df_beam  = torch.load('unique_hits_beam.pt')\n",
    "hits_df_evv   = torch.load('unique_hits_evv.pt')\n",
    "hits_dfs = [hits_df_eee, hits_df_eeevv, hits_df_evv, hits_df_beam]\n",
    "plot_purity_vs_hitcount_multi(dfs, hits_dfs, labels, colors, linestyles, start_hit = 6, max_hits = 20, michel_max = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purity, Efficiency, Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save MULTI; eee; purity, efficiency, threshold\n",
    "def compute_purity(y_true, y_probs, thresholds):\n",
    "    \"\"\"Compute purity (precision) at each threshold.\"\"\"\n",
    "    purity = []\n",
    "    for thresh in thresholds:\n",
    "        y_pred = (y_probs >= thresh).astype(int)\n",
    "        TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        if TP + FP > 0:\n",
    "            purity.append(TP / (TP + FP))\n",
    "        else:\n",
    "            purity.append(np.nan)\n",
    "    return np.array(purity)\n",
    "\n",
    "# Compute purity for each threshold\n",
    "test_thresholds_e = results_electrons['sampled_thresholds'] # replace with(/out) _IC if wanted\n",
    "test_efficiency_e = results_electrons['efficiency'] #\n",
    "test_y_true_e = results_electrons['y_true'] #\n",
    "test_y_probs_e = results_electrons['y_probs'] #\n",
    "test_purity_e = compute_purity(test_y_true_e, test_y_probs_e, test_thresholds_e)\n",
    "\n",
    "# Compute purity for each threshold\n",
    "test_thresholds_p = results_positrons['sampled_thresholds'] #\n",
    "test_efficiency_p = results_positrons['efficiency'] #\n",
    "test_y_true_p = results_positrons['y_true'] #\n",
    "test_y_probs_p = results_positrons['y_probs'] #\n",
    "test_purity_p = compute_purity(test_y_true_p, test_y_probs_p, test_thresholds_p)\n",
    "\n",
    "# Plot Purity vs Threshold\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.plot(test_thresholds_e[:-3], test_purity_e[:-3], label='Electrons', color='blue', lw=2)\n",
    "plt.plot(test_thresholds_p[:-3], test_purity_p[:-3], label='Positrons', color='red', lw=2)\n",
    "\n",
    "# plt.scatter(test_thresholds_p[::10], test_purity_p[::10], label='Positrons',\n",
    "#             color='red', marker='+', s=50, zorder=3)\n",
    "plt.xlabel(\"Required Score\", fontsize=14)\n",
    "plt.ylabel(\"Purity\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "# plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(True)\n",
    "plt.ylim(0.88, 1.002)\n",
    "plt.xlim(-0.02, 1.02)\n",
    "plt.show()\n",
    "\n",
    "# Plot Efficiency vs Threshold\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "# Plot Validation Set as a continuous line\n",
    "plt.plot(test_thresholds_e, test_efficiency_e, label='Electrons', color='blue', lw=2)\n",
    "plt.plot(test_thresholds_p, test_efficiency_p, label='Positrons', color='red', lw=2)\n",
    "\n",
    "# plt.scatter(test_thresholds_p[::50], test_efficiency_p[::50], label='Positrons',\n",
    "#             color='red', marker='+', s=50, zorder=3)\n",
    "plt.xlabel(\"Required Score\", fontsize=14)\n",
    "plt.ylabel(\"Efficiency\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.legend(loc=\"best\", fontsize=11)\n",
    "plt.grid(True)\n",
    "plt.ylim(0.795, 1.005)\n",
    "plt.xlim(0.75, 1.005)\n",
    "plt.show()\n",
    "\n",
    "# Plot Purity vs Efficiency\n",
    "# Define target efficiency values at 0, 0.1, 0.2, ..., 1.0.\n",
    "target_eff = np.linspace(0, 1, 21)\n",
    "# For positrons, find the index of the closest value in test_efficiency_p for each target.\n",
    "selected_indices_p = [np.argmin(np.abs(test_efficiency_p - te)) for te in target_eff]\n",
    "\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.plot(test_efficiency_e, test_purity_e, label='Electrons', color='blue', lw=2)\n",
    "plt.plot(test_efficiency_p, test_purity_p, label='Positrons', color='red', lw=2)\n",
    "\n",
    "# plt.scatter(test_efficiency_p[selected_indices_p], test_purity_p[selected_indices_p], \n",
    "#             label='Positrons', color='red', marker='+', s=50, zorder=3)\n",
    "plt.xlabel(\"Efficiency\", fontsize=14)\n",
    "plt.ylabel(\"Purity\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.legend(loc=\"lower left\", fontsize=11)\n",
    "plt.grid(True)\n",
    "plt.ylim(0.95, 1.001)\n",
    "plt.xlim(0.65, 1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot information for efficiency, purity, threshold\n",
    "# find results (return y_true, y_probs, y_pred_final) for all datasets below\n",
    "\n",
    "results_eee   = evaluate_combined(model, d_test_loader,      dataset_name=\"eee Set\",    num_thresholds=1000)\n",
    "results_IC    = evaluate_combined(model, d_test_loader_IC,   dataset_name=\"eeevv Set\",  num_thresholds=1000)\n",
    "results_beam  = evaluate_combined(model, d_test_loader_beam, dataset_name=\"beam Set\",  num_thresholds=1000)\n",
    "results_michel= evaluate_combined(model, d_test_loader_M,    dataset_name=\"michel Set\",num_thresholds=1000)\n",
    "\n",
    "opt_th_eee    = results_eee   [\"optimal_threshold\"]\n",
    "y_true_eee    = results_eee   [\"y_true\"]\n",
    "y_probs_eee   = results_eee   [\"y_probs\"]\n",
    "\n",
    "opt_th_IC     = results_IC    [\"optimal_threshold\"]\n",
    "y_true_IC     = results_IC    [\"y_true\"]\n",
    "y_probs_IC    = results_IC    [\"y_probs\"]\n",
    "\n",
    "opt_th_beam   = results_beam  [\"optimal_threshold\"]\n",
    "y_true_beam   = results_beam  [\"y_true\"]\n",
    "y_probs_beam  = results_beam  [\"y_probs\"]\n",
    "\n",
    "opt_th_michel = results_michel[\"optimal_threshold\"]\n",
    "y_true_michel = results_michel[\"y_true\"]\n",
    "y_probs_michel= results_michel[\"y_probs\"]\n",
    "\n",
    "# also for each class:\n",
    "# For the eee set (using d_test_loader for electrons and positrons, and for fakes as well)\n",
    "results_electrons_eee = evaluate_for_class(model, d_test_loader, class_index=1, dataset_name=\"Signal Set - Electrons\", num_thresholds=1000)\n",
    "results_positrons_eee = evaluate_for_class(model, d_test_loader, class_index=0, dataset_name=\"Signal Set - Positrons\", num_thresholds=1000)\n",
    "results_fakes_eee = evaluate_for_class(model, d_test_loader, class_index=2, dataset_name=\"Signal Set - Fakes\", num_thresholds=1000)\n",
    "\n",
    "# For the Internal Conversion (IC) set (using d_test_loader_IC)\n",
    "results_electrons_IC = evaluate_for_class(model, d_test_loader_IC, class_index=1, dataset_name=\"IC Set - Electrons\", num_thresholds=1000)\n",
    "results_positrons_IC = evaluate_for_class(model, d_test_loader_IC, class_index=0, dataset_name=\"IC Set - Positrons\", num_thresholds=1000)\n",
    "results_fakes_IC = evaluate_for_class(model, d_test_loader_IC, class_index=2, dataset_name=\"IC Set - Fakes\", num_thresholds=1000)\n",
    "\n",
    "# For the Michel (M) set (using d_test_loader_M)\n",
    "results_electrons_M = evaluate_for_class(model, d_test_loader_M, class_index=1, dataset_name=\"Michel Set - Electrons\", num_thresholds=1000)\n",
    "results_positrons_M = evaluate_for_class(model, d_test_loader_M, class_index=0, dataset_name=\"Michel Set - Positrons\", num_thresholds=1000)\n",
    "# results_fakes_M = evaluate_for_class(model, d_test_loader_M, class_index=2, dataset_name=\"Michel Set - Fakes\", num_thresholds=1000)\n",
    "\n",
    "results_electrons_beam = evaluate_for_class(model, d_test_loader_beam, class_index=1, dataset_name=\"Beam Set - Electrons\", num_thresholds=1000)\n",
    "results_positrons_beam = evaluate_for_class(model, d_test_loader_beam, class_index=0, dataset_name=\"Beam Set - Positrons\", num_thresholds=1000)\n",
    "results_fakes_beam = evaluate_for_class(model, d_test_loader_beam, class_index=2, dataset_name=\"Beam Set - Fakes\", num_thresholds=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_efficiency(y_true, y_probs, thresholds):\n",
    "    \"\"\"Compute efficiency (TPR) at each threshold.\"\"\"\n",
    "    eff = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_probs >= t).astype(int)\n",
    "        TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        FN = np.sum((y_pred == 0) & (y_true == 1))\n",
    "        eff.append(TP / (TP + FN) if (TP + FN) > 0 else np.nan)\n",
    "    return np.array(eff)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define thresholds from 0 to 1.\n",
    "thresholds = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Compute efficiency (TPR) for each dataset.\n",
    "eff_eee   = compute_efficiency(y_true_eee, y_probs_eee, thresholds)\n",
    "eff_IC    = compute_efficiency(y_true_IC, y_probs_IC, thresholds)\n",
    "eff_beam  = compute_efficiency(y_true_beam, y_probs_beam, thresholds)\n",
    "eff_michel = compute_efficiency(y_true_michel, y_probs_michel, thresholds)\n",
    "\n",
    "colors = plt.get_cmap(\"Set2\").colors  # This returns a tuple of color values.\n",
    "plt.figure(figsize=(4.5,3))\n",
    "plt.plot(thresholds, eff_eee, linestyle='-', label=\"Signal\", color='blue', lw=2)\n",
    "plt.plot(thresholds, eff_IC, linestyle='--',label=\"I.C.\", color='orange', lw=2)\n",
    "plt.plot(thresholds, eff_michel, linestyle=':',label=\"Michel\", color='red', lw=2)\n",
    "plt.plot(thresholds, eff_beam, linestyle='-.',label=\"Beam\", color='green', lw=2)\n",
    "plt.xlabel(\"Required Score\", fontsize=14)\n",
    "plt.ylabel(\"Efficiency\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "# plt.legend(loc=\"best\", fontsize=11)\n",
    "plt.grid(True)\n",
    "plt.ylim(0.895, 1.005)\n",
    "plt.xlim(0.745, 1.005)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_purity(y_true, y_probs, thresholds):\n",
    "    \"\"\"Compute purity (precision) at each threshold.\"\"\"\n",
    "    purity = []\n",
    "    for t in thresholds:\n",
    "        y_pred = (y_probs >= t).astype(int)\n",
    "        TP = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        FP = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        purity.append(TP / (TP + FP) if (TP + FP) > 0 else np.nan)\n",
    "    return np.array(purity)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define thresholds from 0 to 1.\n",
    "thresholds = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Compute purity for each dataset.\n",
    "purity_eee   = compute_purity(y_true_eee, y_probs_eee, thresholds)\n",
    "purity_IC    = compute_purity(y_true_IC, y_probs_IC, thresholds)\n",
    "purity_beam  = compute_purity(y_true_beam, y_probs_beam, thresholds)\n",
    "purity_michel = compute_purity(y_true_michel, y_probs_michel, thresholds)\n",
    "\n",
    "# Use ColorBrewer’s Set2 palette for consistent colors.\n",
    "colors = plt.get_cmap(\"Set2\").colors\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4.5,3))\n",
    "\n",
    "max_thresh = 0.999\n",
    "mask = thresholds <= max_thresh\n",
    "\n",
    "thresholds = thresholds[mask]\n",
    "purity_eee = purity_eee[mask]\n",
    "purity_IC = purity_IC[mask]\n",
    "purity_beam = purity_beam[mask]\n",
    "purity_michel = purity_michel[mask]\n",
    "\n",
    "plt.plot(thresholds[:-5], purity_eee[:-5], linestyle='-', label=\"Signal\", color='blue', lw=2)\n",
    "plt.plot(thresholds[:-5], purity_IC[:-5], linestyle='--', label=\"I.C.\", color='orange', lw=2)\n",
    "plt.plot(thresholds[:-5], purity_michel[:-5], linestyle=':', label=\"Michel\", color='red', lw=2)\n",
    "plt.plot(thresholds[:-5], purity_beam[:-5], linestyle='-.', label=\"Beam\", color='green', lw=2)\n",
    "\n",
    "# plt.plot(thresholds, purity_eee, linestyle='-', label=\"Signal Set\", color='orange', lw=2)\n",
    "# plt.plot(thresholds, purity_IC, linestyle='--', label=\"Internal Conversion Set\", color='green', lw=2)\n",
    "# # plt.plot(thresholds, purity_beam, linestyle='-.', label=\"Beam Set\", color='blue', lw=2)\n",
    "# plt.plot(thresholds, purity_michel, linestyle=':', label=\"Michel Set\", color='purple', lw=2)\n",
    "plt.xlabel(\"Required Score\", fontsize=14)\n",
    "plt.ylabel(\"Purity\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "# plt.legend(loc=\"best\", fontsize=11, bbox_to_anchor=(1, 0.5))\n",
    "plt.grid(True)\n",
    "plt.ylim(0.395, 1.02)\n",
    "plt.ylim(0.945, 0.99)\n",
    "plt.xlim(-0.02, 1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence histograms for each class\n",
    "# if the GNN is confident in a graph being fake, it will have a high score in class 2, but for the plot below\n",
    "# it applies (1 - score) so that the fake peak is near 0.0\n",
    "from GNNplots import plot_confidence_histograms_multi\n",
    "\n",
    "plot_confidence_histograms_multi(\"Signal \",                  results_electrons_eee,   results_positrons_eee,   results_fakes_eee)\n",
    "plot_confidence_histograms_multi(\"I.C. \",          results_electrons_IC,    results_positrons_IC,    results_fakes_IC)\n",
    "# plot_confidence_histograms_multi(\"Michel\",        results_electrons_M,     results_positrons_M,     fake_results=None)\n",
    "# plot_confidence_histograms_multi(\"Beam\",          results_electrons_beam,  results_positrons_beam,  results_fakes_beam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed confidence histograms for each decay type\n",
    "from GNNplots import plot_class_histograms_with_errors\n",
    "\n",
    "# Example usage for electrons:\n",
    "dataset_labels_elec = [\"Signal – e⁻\", \"I.C. – e⁻\", \"Michel – e⁻\", \"Beam – e⁻\"]\n",
    "elec_results_list   = [\n",
    "    results_electrons_eee,\n",
    "    results_electrons_IC,\n",
    "    results_electrons_M,\n",
    "    results_electrons_beam\n",
    "]\n",
    "colors      = [\"blue\", \"orange\", \"red\", \"green\"]\n",
    "line_styles = [\"-\", \"--\", \":\", \"-.\"]\n",
    "\n",
    "plot_class_histograms_with_errors(\n",
    "    dataset_labels_elec,\n",
    "    elec_results_list,\n",
    "    axis_label=\"GNN output for electrons\",\n",
    "    y_label=\"Frequency Density\",\n",
    "    color_list=colors,\n",
    "    style_list=line_styles,\n",
    "    ylim=(2e-3, 260)\n",
    ")\n",
    "\n",
    "# Example usage for positrons:\n",
    "dataset_labels_pos = [\"Signal – e⁺\", \"I.C. – e⁺\", \"Michel – e⁺\", \"Beam – e⁺\"]\n",
    "pos_results_list   = [\n",
    "    results_positrons_eee,\n",
    "    results_positrons_IC,\n",
    "    results_positrons_M,\n",
    "    results_positrons_beam\n",
    "]\n",
    "\n",
    "plot_class_histograms_with_errors(\n",
    "    dataset_labels_pos,\n",
    "    pos_results_list,\n",
    "    axis_label=\"GNN output for positrons\",\n",
    "    y_label=\"Frequency Density\",\n",
    "    color_list=colors,\n",
    "    style_list=line_styles,\n",
    "    ylim=(2e-3, 170)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
