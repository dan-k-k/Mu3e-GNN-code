{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, BatchNorm, GINEConv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, average_precision_score, precision_recall_curve, roc_curve,\n",
    "    classification_report, auc\n",
    ")\n",
    "from sklearn.utils import resample\n",
    "import math\n",
    "import random\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Random Seed for Reproducibility (keep as 42)\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)  # 'None' removes the limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at available trees/classes and branches. \n",
    "# load both segs and mc_tracks for all four datasets (signal, internal conversion, michel and beam).\n",
    "# ---segs--- gives the MC triplet information which can be used to find individual hits in a frame, later used to make graphs. \n",
    "# ---mc_tracks--- gives information of tracks found by the standard reconstruction algorithm (used only for final comparison).\n",
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "root_file_path   = 'mu3e_5E4_signal_reco.root'\n",
    "root_file_path2  = 'mu3e_reco_2E4_alltrk_IC.root'\n",
    "root_file_path3  = 'mu3e_reco_5E4_beam.root'\n",
    "root_file_path4  = 'mu3e_reco_5E4_alltrk_michel.root'\n",
    "\n",
    "# Open files\n",
    "file_signal = uproot.open(root_file_path)\n",
    "file_IC     = uproot.open(root_file_path2)\n",
    "file_beam   = uproot.open(root_file_path3)\n",
    "file_michel = uproot.open(root_file_path4)\n",
    "\n",
    "# Print available trees/classes\n",
    "print(f\"Signal file classes:   {file_signal.classnames()}\")\n",
    "print(f\"IC     file classes:   {file_IC.classnames()}\")\n",
    "print(f\"Beam   file classes:   {file_beam.classnames()}\")\n",
    "print(f\"Michel file classes:   {file_michel.classnames()}\\n\")\n",
    "\n",
    "# Select trees from each dataset\n",
    "segs10     = file_signal['segs;10'] # signal\n",
    "frames1    = file_signal['frames;1']\n",
    "mc_tracks1 = file_signal['mc_tracks;1']\n",
    "\n",
    "segsIC     = file_IC['segs;6'] # internal conversion\n",
    "mc_tracksIC = file_IC['mc_tracks;1']\n",
    "\n",
    "segsbeam   = file_beam['segs;24'] # beam\n",
    "mc_tracksbeam = file_beam['mc_tracks;2']\n",
    "\n",
    "segsmichel = file_michel['segs;4'] # Michel\n",
    "mc_tracksM = file_michel['mc_tracks;1']\n",
    "\n",
    "# Print available branches\n",
    "print(\"Branches in 'segs;10' (signal):      \", segs10.keys())\n",
    "print(\"Branches in 'frames;1' (signal):     \", frames1.keys())\n",
    "print(\"Branches in 'mc_tracks;1' (signal):  \", mc_tracks1.keys(), \"\\n\")\n",
    "\n",
    "# Define which segs branches to load\n",
    "segs_branches = [\n",
    "    'x00','x10','x20', # triplet hit positions (x,y,z)\n",
    "    'y00','y10','y20',\n",
    "    'z00','z10','z20',\n",
    "    'frameId','mc_tid','mc_pid','mc_type','mc_p','mc_pt',\n",
    "    'mc_phi','mc_lam','mc_theta'\n",
    "    # frame id – usually extends to a round number eg 50,000 or 20,000, \n",
    "    # track id – an identifier, where all hits of an MC track are assigned the same value,\n",
    "    # particle id – 11 for electron, -11 for positron, \n",
    "    # type – the decay the particles originate from (92 is mu to eee, i'm unsure on the rest), \n",
    "    # momentum p, \n",
    "    # transverse momentum pT, \n",
    "    # azimuthal angle phi – about the z axis, \n",
    "    # pitch angle lambda – from the x,y plane projected onto +z direction, \n",
    "    # theta and phi histograms look identical but flipped on x-axis.\n",
    "    # note: when hits have tid, type, p, pT etc = 0, it is noise.\n",
    "    #       noise hits can sometimes be shared with real MC hits, so any code \n",
    "    #       that finds all hits in a frame must save hit info if available.\n",
    "]\n",
    "frames_branches = ['frameId','mc_tid','x0','y0','z0','t0']\n",
    "    # x0, y0, z0 is the position of the very first hit, needed for first_hit flagging when finding 8hit graphs.\n",
    "tracks_branches = [\n",
    "    'runId','frameId','n','type','pid','tid','nhit','nfb','nfbc',\n",
    "    'ntl','n3','n4','n6','n8','p','pt','phi','lam','theta'\n",
    "    # n6 tells you whether a MC track (id) has at least 6 hits.\n",
    "    # n8 tells you whether a MC track (id) has at least 8 hits.\n",
    "    #   n4, n6, n8 are usually 1 or 0, but can be >1 if the track recurls.\n",
    "    # ntl tells you if a particle reaches the scintillation tile (ie. it is a 6hit track in the forward recurl layers).\n",
    "    # --each row in mc_tracks is of a unique frame id.\n",
    "    # --used for final efficiency comparison of standard reco against GNN.\n",
    "]\n",
    "\n",
    "# Load the segs DataFrames\n",
    "segs10_data      = segs10.arrays(segs_branches,   library='pd')\n",
    "frames1_data     = frames1.arrays(frames_branches, library='pd')\n",
    "\n",
    "segsIC_data      = segsIC.arrays(segs_branches,   library='pd')\n",
    "segsmichel_data  = segsmichel.arrays(segs_branches, library='pd')\n",
    "\n",
    "segsbeam_data    = segsbeam.arrays(segs_branches, library='pd')\n",
    "# limit to first 4000 unique frames for beam\n",
    "first4k = np.sort(segsbeam_data['frameId'].unique())[:4000]\n",
    "segsbeam_data    = segsbeam_data[segsbeam_data['frameId'].isin(first4k)].reset_index(drop=True)\n",
    "\n",
    "# Now load mc_tracks DataFrames (all branches)\n",
    "mc_tracks1_data    = mc_tracks1.arrays(tracks_branches, library='pd')\n",
    "mc_tracksIC_data   = mc_tracksIC.arrays(tracks_branches, library='pd')\n",
    "mc_tracksbeam_data = mc_tracksbeam.arrays(tracks_branches, library='pd')\n",
    "# apply the *same* first4k list from segsbeam_data to mc_tracksbeam_data\n",
    "mc_tracksbeam_data = mc_tracksbeam_data[\n",
    "    mc_tracksbeam_data['frameId'].isin(first4k)].reset_index(drop=True)\n",
    "mc_tracksM_data    = mc_tracksM.arrays(tracks_branches, library='pd')\n",
    "\n",
    "# Peek at each DataFrame (signal)\n",
    "print(\"segs10_data.head():\\n\", segs10_data.head(), \"\\n\")\n",
    "print(\"frames1_data.head():\\n\", frames1_data.head(), \"\\n\")\n",
    "print(\"mc_tracks1_data.head():\\n\", mc_tracks1_data.head(15), \"\\n\")\n",
    "\n",
    "print(\"Lowest frameId in segs10_data:\", segs10_data['frameId'].min(), \"to highest\", segs10_data['frameId'].max())\n",
    "print(\"Lowest frameId in segsIC_data:\", segsIC_data['frameId'].min(), \"to highest\", segsIC_data['frameId'].max())\n",
    "print(\"Lowest frameId in segsbeam_data:\", segsbeam_data['frameId'].min(), \"to highest\", segsbeam_data['frameId'].max())\n",
    "print(\"Lowest frameId in segsmichel_data:\", segsmichel_data['frameId'].min(), \"to highest\", segsmichel_data['frameId'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv the tracks found by standard reconstruction\n",
    "# this is simply a list of mc_tids and frameIds of all tracks in mc_tracks\n",
    "import pandas as pd\n",
    "\n",
    "_list_cols = ['tid', 'n6', 'n8', 'ntl']\n",
    "\n",
    "def explode_and_save(df: pd.DataFrame, out_csv: str):\n",
    "    # 1) Copy and turn any awkward/list-like column into a real Python list\n",
    "    df2 = df.copy()\n",
    "    for col in _list_cols:\n",
    "        if col in df2.columns:\n",
    "            df2[col] = df2[col].apply(\n",
    "                lambda x: list(x) if hasattr(x, \"tolist\") or isinstance(x, (tuple, pd.Series)) else x\n",
    "            )\n",
    "\n",
    "    # 2) Build a zipped column of tuples, one tuple per “candidate”\n",
    "    #    (zip will automatically pad shorter lists by dropping extra elements)\n",
    "    df2['_zipped'] = df2[_list_cols].apply(lambda row: list(zip(*row)), axis=1)\n",
    "\n",
    "    # 3) Explode that one column\n",
    "    df3 = df2.explode('_zipped')\n",
    "\n",
    "    # 4) Unpack the tuples back into the original columns\n",
    "    for i, col in enumerate(_list_cols):\n",
    "        df3[col] = df3['_zipped'].str[i]\n",
    "\n",
    "    df3 = df3.drop(columns=['_zipped'])\n",
    "\n",
    "    # 5) Apply selection\n",
    "    sel = (\n",
    "        df3.loc[\n",
    "            (df3['n6'] == 1) &\n",
    "            (df3['n8'] == 0) &\n",
    "            (df3['ntl'] > 0),\n",
    "            ['tid', 'frameId']\n",
    "        ]\n",
    "        .drop_duplicates()\n",
    "        .rename(columns={'tid': 'mc_tid'})\n",
    "    )\n",
    "    \n",
    "    # cast mc_tid (and frameId) to integers\n",
    "    sel['mc_tid']   = sel['mc_tid'].astype(int)\n",
    "    sel['frameId'] = sel['frameId'].astype(int)\n",
    "\n",
    "    sel.to_csv(out_csv, index=False)\n",
    "    print(f\"→ wrote {len(sel)} rows to {out_csv}\")\n",
    "\n",
    "# Call for each mc_tracks DataFrame:\n",
    "explode_and_save(mc_tracks1_data,    \"stdreco_6hit_tracks_signal1.csv\")\n",
    "explode_and_save(mc_tracksIC_data,   \"stdreco_6hit_tracks_IC1.csv\")\n",
    "explode_and_save(mc_tracksbeam_data, \"stdreco_6hit_tracks_beam1.csv\")\n",
    "explode_and_save(mc_tracksM_data,    \"stdreco_6hit_tracks_michel1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hits_df_unique contains the unique hits that are used to generate graphs. \n",
    "\n",
    "tracks_df contains the truth tracks determined before any track generation (built from triplets so have length of at least three). \n",
    "\n",
    "sixhit_tracks_df finds the subset of these truth tracks that follow the layer sequences 1 > 2 > 3 > 4 > 4+- > 3+- where +- is for the forward recurl layers far into +-z direction. \n",
    "these are exactly the tracks that the 6-hit graph generating algorithm attempts to find.\n",
    "\n",
    "real_tracks_df and fake_tracks_df are from the 'validation' (constraints) algorithm which either do or dont match all of their track ids within the built track that follows the same layer sequence as above. \n",
    "'real' and 'fake' tracks are found from validated_tracks_df, which contains all tracks that pass constraints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find truth tracks for later comparison/plots/plotly 3D view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'tracks_df' is a dataframe containing all tracks that can be built from segs.\n",
    "# tracks include anything with more than three hits (from triplets)\n",
    "# there are cases where a built truth sixhit track has all mc_tids=0 (these are discarded entirely in the later method before turning into graphs)\n",
    "\n",
    "# 'sixhit_tracks_df' filters only for tracks that follow the sequences 1,2,3,4,4+-,3+-\n",
    "#  where +- is the forward recurl layer far into +-z direction\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from truetrackbuilding import explode_triplets_to_tracks, assign_layer1\n",
    "\n",
    "# run for segs10_data, segsIC_data, segsbeam_data, segsmichel_data\n",
    "tracks_df = explode_triplets_to_tracks(segs10_data) # here\n",
    "tracks_df = tracks_df[tracks_df['mc_tid'] != 0].copy()\n",
    "print(f\"Built {len(tracks_df):,} total tracks (length ≥3 hits).\")\n",
    "tracks_df[\"layer_sequence\"] = tracks_df[\"hits\"].apply(lambda hits_list: [assign_layer1(hit) for hit in hits_list])\n",
    "\n",
    "# Filters only + and - recurls from the very first recorded hit.\n",
    "desired_sequences = [\n",
    "    ['1', '2', '3', '4', '4+', '3+'],\n",
    "    ['1', '2', '3', '4', '4-', '3-'],\n",
    "]\n",
    "\n",
    "sixhit_candidates = tracks_df[ tracks_df[\"hits\"].apply(len) == 6 ].copy()\n",
    "# there are rare instances of a subsequence that matches the desired sequence (due to MS) but these \n",
    "# do not count as 6hits, so applying len==6 is fine.\n",
    "sixhit_candidates = sixhit_candidates[\n",
    "    sixhit_candidates[\"layer_sequence\"].apply(lambda seq: seq in desired_sequences)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "# Select relevant columns\n",
    "sixhit_tracks_df = sixhit_candidates[[\"frameId\", \"mc_tid\", \"mc_pid\", \"mc_type\", \"mc_p\", \"mc_pt\", \"layer_sequence\", \"hits\"]].copy()\n",
    "\n",
    "# Display a summary of the final filtered and truncated tracks\n",
    "print(\"\\nFinal Filtered 6hit Tracks:\")\n",
    "print(sixhit_tracks_df.head())\n",
    "print(f\"Built {len(sixhit_tracks_df):,} six-hit recurl tracks.\")\n",
    "\n",
    "torch.save(tracks_df, \"true_tracks_eee_May31.pt\") # also change name here\n",
    "torch.save(sixhit_tracks_df, \"sixhit_tracks_eee_May31.pt\") # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of the true unique tids and frameids for final efficiency comparisons for 6hit tracks/graphs\n",
    "import pandas as pd\n",
    "\n",
    "filtered_df = sixhit_tracks_df[sixhit_tracks_df['mc_tid'] != 0]\n",
    "unique_df = filtered_df[['mc_tid', 'frameId']].drop_duplicates()\n",
    "# unique_df.to_csv('true6hitmctidsframeidsMichel.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the loosest constraints that will 'validate' all real tracks\n",
    "# here you will see how the z12/z34 constraint is very bad and needs to be looser\n",
    "# or replace the constraint with something else entirely.\n",
    "sixhit_tracks_df = torch.load(\"sixhit_tracks_eee_May31.pt\") #\n",
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from build6hittracks import collect_sixhit_distributions\n",
    "\n",
    "# constraints contains the histogram information for each constraint used in graph generation (track 'validation')\n",
    "constraints = collect_sixhit_distributions(sixhit_tracks_df)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pull each list out of the dictionary\n",
    "d12_list      = constraints[\"d12\"]\n",
    "d34_list      = constraints[\"d34\"]\n",
    "d56_list      = constraints[\"d56\"]\n",
    "zratio56_list   = constraints[\"zratio56\"]\n",
    "zratio12_list   = constraints[\"zratio12\"]\n",
    "center_diff_list  = constraints[\"center_diff\"]\n",
    "radius_diff_list  = constraints[\"radius_diff\"]\n",
    "pitch_diff_list   = constraints[\"pitch_diff\"]\n",
    "\n",
    "# Re‐assemble into a mapping from label → data, exactly as before:\n",
    "constraints_for_plot = {\n",
    "    'N$_1$–N$_2$ Transverse Distance [mm]':      d12_list,\n",
    "    'N$_3$–N$_4$ Transverse Distance [mm]':      d34_list,\n",
    "    'N$_5$–N$_6$ Transverse Distance [mm]':      d56_list,\n",
    "    'Min. Centre Transverse Difference [mm]':    center_diff_list,\n",
    "    'Radius Tolerance [mm]':                     radius_diff_list,\n",
    "    'Min. Pitch Difference [mm/rad]':            pitch_diff_list,\n",
    "    'z$_{5-6}$/z$_{3-4}$ [%]':                   zratio56_list,\n",
    "    'z$_{1-2}$/z$_{3-4}$ [%]':                   zratio12_list,\n",
    "}\n",
    "\n",
    "x_ranges = {\n",
    "    'N$_1$–N$_2$ Transverse Distance [mm]':   (0, 30),\n",
    "    'N$_3$–N$_4$ Transverse Distance [mm]':   (0, 60),\n",
    "    'N$_5$–N$_6$ Transverse Distance [mm]':   (0, 60),\n",
    "    'Min. Centre Transverse Difference [mm]': (0, 100),\n",
    "    'Radius Tolerance [mm]':                  (0, 50),\n",
    "    'Min. Pitch Difference [mm/rad]':         (0, 40),\n",
    "    'z$_{5-6}$/z$_{3-4}$ [%]':                (0, 200),\n",
    "    'z$_{1-2}$/z$_{3-4}$ [%]':                (0, 150)\n",
    "}\n",
    "\n",
    "for label, data in constraints_for_plot.items():\n",
    "    arr = np.array(data)\n",
    "    arr = arr[np.isfinite(arr)]   # filter out any NaNs\n",
    "\n",
    "    plt.figure(figsize=(4.5, 3))\n",
    "    if label in x_ranges:\n",
    "        lo, hi = x_ranges[label]\n",
    "        plt.hist(arr, bins=50, range=(lo, hi),\n",
    "                 histtype='step', color='k', linewidth=1.5)\n",
    "        plt.xlim(lo, hi)\n",
    "    else:\n",
    "        plt.hist(arr, bins=50, histtype='step', color='k', linewidth=1.5)\n",
    "\n",
    "    # draw any red dashed lines:\n",
    "    if label == 'Min. Centre Transverse Difference [mm]':\n",
    "        plt.axvline(50, color='red', linestyle='--', linewidth=1.5,\n",
    "                    label='set to 50 mm')\n",
    "        plt.legend(fontsize=11)\n",
    "\n",
    "    if label == 'z$_{5-6}$/z$_{3-4}$ [%]':\n",
    "        plt.axvline(35, color='red', linestyle='--', linewidth=1.5,\n",
    "                    label='set to ±65%')\n",
    "        plt.axvline(165, color='red', linestyle='--', linewidth=1.5)\n",
    "        plt.legend(fontsize=11, framealpha=0.3)\n",
    "\n",
    "    #  Here is is for z_{1-2}/z_{3-4} =\n",
    "    #  “0.4 ± 0.65” ⇒ [14%, 66%]\n",
    "    if label == 'z$_{1-2}$/z$_{3-4}$ [%]':\n",
    "        plt.axvline(14, color='red', linestyle='--', linewidth=1.5,\n",
    "                    label='set to 40%×(1−0.65) = 14%')\n",
    "        plt.axvline(66, color='red', linestyle='--', linewidth=1.5,\n",
    "                    label='set to 40%×(1+0.65) = 66%')\n",
    "        plt.legend(fontsize=11, framealpha=0.3)\n",
    "\n",
    "    plt.xlabel(label, fontsize=14)\n",
    "    plt.ylabel(\"Frequency\", fontsize=14)\n",
    "    plt.yscale('log')\n",
    "    plt.tick_params(labelsize=12)\n",
    "    plt.grid(linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all truth track info for a frame\n",
    "frame_id = 4202\n",
    "tracks_in_frame = tracks_df[tracks_df['frameId'] == frame_id]\n",
    "print(tracks_in_frame[['mc_tid', 'layer_sequence', 'hits']])\n",
    "from IPython.display import HTML\n",
    "HTML('<div style=\"height:300px; overflow:auto\">' + tracks_in_frame.to_html() + '</div>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find pT the MS effect is minimised (ie where the recurl hits occur near phi= pi ±0.5 rad) \n",
    "# Fig 2.4 in the TDR\n",
    "import numpy as np\n",
    "from circlefitanalysis import recurl_angle\n",
    "\n",
    "tol = 0.5\n",
    "dphis = []\n",
    "mask = []\n",
    "\n",
    "for hits in tracks_df['hits']:\n",
    "    dphi = recurl_angle(hits)\n",
    "    dphis.append(dphi)\n",
    "    mask.append(abs(abs(dphi) - np.pi) < tol)\n",
    "\n",
    "tracks_df['dphi_recurl'] = dphis\n",
    "recurling = tracks_df[mask]\n",
    "\n",
    "print(f\"Found {len(recurling)} tracks with Δφ≈π ± {tol} rad\")\n",
    "\n",
    "pt_recurl = tracks_df.loc[mask, 'mc_pt']\n",
    "\n",
    "counts, bin_edges = np.histogram(pt_recurl, bins=25)\n",
    "bin_centers = 0.5*(bin_edges[:-1] + bin_edges[1:])\n",
    "errors = np.sqrt(counts)\n",
    "\n",
    "plt.figure(figsize=(6,4.5))\n",
    "plt.hist(pt_recurl,\n",
    "         bins=bin_edges,\n",
    "         histtype='step',\n",
    "         linewidth=1.4,\n",
    "         color='navy',\n",
    "         label='Recurling Tracks')\n",
    "plt.errorbar(bin_centers, counts,\n",
    "             yerr=errors,\n",
    "             fmt='none',\n",
    "             ecolor='black',\n",
    "             capsize=3)\n",
    "\n",
    "plt.xlabel(r\"$p_{\\mathrm{T,true}}$ [MeV]\", fontsize=14)\n",
    "plt.ylabel(r\"Number of Tracks ($\\Delta\\phi \\approx \\pi \\pm 0.5\\,$rad)\", fontsize=14)\n",
    "plt.tick_params(axis=\"both\", labelsize=12)\n",
    "plt.grid(linestyle=\"--\", alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# the high pT tail does not make sense to me. i viewed these frames/trackids in the frame \n",
    "# plotly and they seemed to have been incorrectly assigned high momenta? they have the\n",
    "# same bending radii as the low pT tracks. or it is after energy loss and only some final hits pass some filter\n",
    "#  use the frame visualisation in the cell below to look at frames with anomalous tracks\n",
    "\n",
    "for _, row in tracks_df[mask & (tracks_df['mc_pt'] > 50)].head(5).iterrows():\n",
    "    print(f\"Frame {row['frameId']}, mc_tid = {row['mc_tid']}, pT = {row['mc_pt']:.1f} MeV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly visualisation to look at frames\n",
    "# eg. here, frame 473 track id 67924 has large pT = 51MeV but appears to have a small bending radius (just large enough to reach layer 4)\n",
    "from plotlyhelper import visualise_truth_tracks, wireframe_traces\n",
    "target_frame_id = 473  # Replace with desired frame id\n",
    "visualise_truth_tracks(target_frame_id, tracks_df, wireframe_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find full and amb info for double hit probability\n",
    "# the code looks at just 6hit track mc_tids\n",
    "# 'full' info refers to tracks that complete a 6hit \n",
    "# whereas 'ambiguous' refers to incomplete tracks that follow the same trajectory with the same mc_tid\n",
    "# but contain a different hit in a speific layer due to overlapping ladders in the detector, leading to double hits.\n",
    "# the plot finds the probability of having double hits for any given true 6hit track. so it is not exactly the same as the probability \n",
    "# of having a duplicate 6hit graph for a given mc_tid which depends on the number of hits given per layer\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# two recurl sequences:\n",
    "desired_sequences = [\n",
    "    ['1', '2', '3', '4', '4+', '3+'],\n",
    "    ['1', '2', '3', '4', '4-', '3-']\n",
    "]\n",
    "\n",
    "# Containers:\n",
    "phi_by_layer = defaultdict(lambda: {'full': [], 'amb': []})\n",
    "full_info = []\n",
    "amb_info  = []\n",
    "\n",
    "# Single pass over all (frameId, mc_tid) groups\n",
    "for (frame_id, mc_tid), group in tracks_df.groupby(['frameId', 'mc_tid']):\n",
    "    # pick the first “full” recurl track if any. the ambiguous (double hit) track does not form a 6hit.\n",
    "    full = group.loc[\n",
    "        group['hits'].apply(len).eq(6) &\n",
    "        group['layer_sequence'].isin(desired_sequences)\n",
    "    ]\n",
    "    amb  = group.loc[group['hits'].apply(len).ne(6)]\n",
    "    \n",
    "    if full.empty or amb.empty:\n",
    "        continue\n",
    "    \n",
    "    track_full = full.iloc[0]\n",
    "    track_amb  = amb .iloc[0]\n",
    "    \n",
    "    # map layer→hit once each\n",
    "    full_hits = {layer:hit for hit, layer in zip(track_full['hits'], track_full['layer_sequence'])}\n",
    "    amb_hits  = {layer:hit for hit, layer in zip(track_amb ['hits'], track_amb ['layer_sequence'])}\n",
    "    \n",
    "    # look at layers in common\n",
    "    for layer in full_hits.keys() & amb_hits.keys():\n",
    "        hf = np.asarray(full_hits[layer])\n",
    "        ha = np.asarray( amb_hits[layer])\n",
    "        \n",
    "        # skip identical within tolerance\n",
    "        if np.allclose(hf, ha, atol=1e-6):\n",
    "            continue\n",
    "        \n",
    "        # compute phis\n",
    "        phi_f = np.arctan2(hf[1], hf[0])\n",
    "        phi_a = np.arctan2(ha[1], ha[0])\n",
    "        \n",
    "        # accumulate per‐layer lists\n",
    "        d = phi_by_layer[layer]\n",
    "        d['full'].append(phi_f)\n",
    "        d['amb' ].append(phi_a)\n",
    "        \n",
    "        # shared metadata\n",
    "        meta = {\n",
    "            'frameId': frame_id,\n",
    "            'mc_tid':  mc_tid,\n",
    "            'mc_pid': track_full.get('mc_pid', None),\n",
    "            'mc_type':track_full.get('mc_type',None),\n",
    "            'mc_p':   track_full.get('mc_p',   None),\n",
    "            'mc_pt':  track_full.get('mc_pt',  None),\n",
    "            'layer':  layer\n",
    "        }\n",
    "        full_info.append({**meta, 'hit': hf, 'phi': phi_f})\n",
    "        amb_info .append({**meta, 'hit': ha, 'phi': phi_a})\n",
    "\n",
    "# duplicated hit probability as fn of pT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Overall (Full) Tracks: restrict to 6-hit recurls ---\n",
    "df_all = tracks_df[tracks_df['hits'].apply(len) == 6]\n",
    "df_all = df_all[df_all['layer_sequence'].apply(lambda seq: seq in desired_sequences)]\n",
    "df_all = df_all.dropna(subset=['mc_pt'])\n",
    "df_all = df_all[df_all['mc_pt'] != 0]\n",
    "df_all = df_all[df_all['mc_tid'].apply(lambda x: isinstance(x, int) and x != 0)]\n",
    "df_all = df_all[~df_all['mc_type'].isin(['Multiple', 'Unknown'])]\n",
    "\n",
    "# --- Duplicate Hits from amb_info (collected from duplicate-checking) ---\n",
    "df_dup = pd.DataFrame(amb_info)\n",
    "df_dup = df_dup.dropna(subset=['mc_pt'])\n",
    "df_dup = df_dup[df_dup['mc_pt'] != 0]\n",
    "df_dup = df_dup[df_dup['mc_tid'].apply(lambda x: isinstance(x, int) and x != 0)]\n",
    "df_dup = df_dup[~df_dup['mc_type'].isin(['Multiple', 'Unknown'])]\n",
    "\n",
    "# --- Split into Electrons and Positrons ---\n",
    "df_all_e = df_all[df_all['mc_pid'] == 11]\n",
    "df_all_p = df_all[df_all['mc_pid'] == -11]\n",
    "print(len(df_all_e), 'e-', len(df_all_p), 'e+')\n",
    "df_dup_e = df_dup[df_dup['mc_pid'] == 11]\n",
    "df_dup_p = df_dup[df_dup['mc_pid'] == -11]\n",
    "\n",
    "# --- Define Common Bins in pT ---\n",
    "# We use a fixed lower bound and the max of all four subsets. \n",
    "pt_max = max(df_all_e['mc_pt'].max(), df_all_p['mc_pt'].max())\n",
    "pt_min = min(df_all_e['mc_pt'].min(), df_all_p['mc_pt'].min())\n",
    "bins = np.linspace(pt_min, pt_max, 31)  # 50 bins\n",
    "bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "\n",
    "# --- Compute Histogram Counts and Duplication Probability for Electrons ---\n",
    "counts_all_e, _ = np.histogram(df_all_e['mc_pt'], bins=bins)\n",
    "counts_dup_e, _ = np.histogram(df_dup_e['mc_pt'], bins=bins)\n",
    "probability_e = np.divide(counts_dup_e, counts_all_e, out=np.zeros_like(counts_dup_e, dtype=float), where=counts_all_e != 0)\n",
    "err_e = np.zeros_like(probability_e, dtype=float)\n",
    "nonzero_e = counts_all_e > 0\n",
    "err_e[nonzero_e] = np.sqrt(probability_e[nonzero_e]*(1 - probability_e[nonzero_e]) / counts_all_e[nonzero_e])\n",
    "\n",
    "# --- Compute Histogram Counts and Duplication Probability for Positrons ---\n",
    "counts_all_p, _ = np.histogram(df_all_p['mc_pt'], bins=bins)\n",
    "counts_dup_p, _ = np.histogram(df_dup_p['mc_pt'], bins=bins)\n",
    "probability_p = np.divide(counts_dup_p, counts_all_p, out=np.zeros_like(counts_dup_p, dtype=float), where=counts_all_p != 0)\n",
    "err_p = np.zeros_like(probability_p, dtype=float)\n",
    "nonzero_p = counts_all_p > 0\n",
    "err_p[nonzero_p] = np.sqrt(probability_p[nonzero_p]*(1 - probability_p[nonzero_p]) / counts_all_p[nonzero_p])\n",
    "\n",
    "# --- Plot the Overlapping Probability Distributions ---\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.errorbar(bin_centers, probability_e, yerr=err_e, fmt='o', color='blue', capsize=3, label='Electrons')\n",
    "plt.errorbar(bin_centers, probability_p, yerr=err_p, fmt='o', color='red', capsize=3, label='Positrons')\n",
    "\n",
    "plt.xlabel(r\"$p_{T}^{\\rm true}$ [MeV]\", fontsize=14)\n",
    "plt.ylabel(\"Double Hit Probability\", fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.legend(fontsize=12, loc='upper center')\n",
    "plt.show()\n",
    "\n",
    "# # finds duplicates with pT above 50MeV?\n",
    "# df_dup_highpt = df_dup[df_dup['mc_pt'] > 50]\n",
    "# print(df_dup_highpt.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotly visualisation\n",
    "target_frame_id = 878  # Replace with desired frame id\n",
    "visualise_truth_tracks(target_frame_id, tracks_df, wireframe_traces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average Number of Hits per Layer\n",
    "# Ensure necessary libraries are imported\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the correct data (hits unique is found later)\n",
    "# hits_df_unique   = torch.load('unique_hits_eee.pt') # unique_hits_eee, unique_hits_eeevv, unique_hits_evv, unique_hits_beam\n",
    "\n",
    "# Step 1: Group by 'frameId' and 'layer' to count hits per layer per frame\n",
    "hits_per_frame_layer = hits_df_unique.groupby(['frameId', 'layer']).size().reset_index(name='hit_count')\n",
    "fmin = hits_per_frame_layer['frameId'].min()\n",
    "fmax = hits_per_frame_layer['frameId'].max()\n",
    "\n",
    "# Step 2: Pivot the DataFrame to have frames as rows and layers as columns\n",
    "all_frames = pd.Index(range(fmin, fmax+1), name='frameId')\n",
    "hits_pivot = (hits_per_frame_layer.pivot(index='frameId', columns='layer', values='hit_count').reindex(all_frames, fill_value=0))\n",
    "# Step 3: Calculate mean and standard deviation for each layer\n",
    "mean_hits = hits_pivot.mean()\n",
    "std_hits = hits_pivot.std()\n",
    "N = len(hits_pivot)  # Number of frames (samples)\n",
    "\n",
    "# Step 4: Compute standard error of the mean (SEM)\n",
    "sem_hits = std_hits / np.sqrt(N)  # Error as sqrt(s²/N)\n",
    "\n",
    "# Step 5: Plotting\n",
    "plt.figure(figsize=(4.5, 4))\n",
    "bar_container = plt.bar(\n",
    "    mean_hits.index,\n",
    "    mean_hits.values,\n",
    "    yerr=sem_hits.values,  # Corrected error bars\n",
    "    capsize=5,\n",
    "    color=sns.color_palette('magma', len(mean_hits)),\n",
    "    # edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add numerical labels on top of each bar with a slight right shift\n",
    "for bar in bar_container:\n",
    "    height = bar.get_height()\n",
    "    width = bar.get_width()\n",
    "    x = bar.get_x()\n",
    "    \n",
    "    # Calculate a small shift (e.g., 5% of the bar width)\n",
    "    shift = width * 0  # Adjust the multiplier (e.g., 0.05 for 5%)\n",
    "    \n",
    "    # Ensure labels do not go beyond the plot's x-axis limits\n",
    "    label_x = x + width / 2 + shift\n",
    "    # Optionally, clamp the label_x to the axis limits\n",
    "    ax_limits = plt.gca().get_xlim()\n",
    "    label_x = min(label_x, ax_limits[1] - width * 0.1)  # Adjust 0.1 as needed\n",
    "    \n",
    "    plt.text(\n",
    "        label_x,\n",
    "        height,\n",
    "        f'{height:.2f}',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=12,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Detector Layer', fontsize=14)\n",
    "plt.ylabel('Average Number of Hits per Frame', fontsize=13)\n",
    "plt.ylim(0, max(mean_hits.values + sem_hits.values) * 1.1)  # Adjust for SEM\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.tick_params(axis='both', labelsize=13)\n",
    "plt.grid(axis='y', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "# Improve layout and display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of truth 6 hit tracks and long tracks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Find full frame‐ID range from segs10_data, segsIC_data, segsbeam_data, segsmichel_data:\n",
    "min_id = int(segs10_data['frameId'].min())\n",
    "max_id = int(segs10_data['frameId'].max())\n",
    "full_frame_index = np.arange(min_id, max_id + 1)  # all integers from min to max\n",
    "\n",
    "# Count six‐hit recurl tracks (mc_tid != 0) per frame:\n",
    "tracks_per_frame = (\n",
    "    sixhit_tracks_df[sixhit_tracks_df['mc_tid'] != 0]\n",
    "    .groupby('frameId')['mc_tid']\n",
    "    .nunique()\n",
    ")\n",
    "# Reindex onto the full range, filling in 0 for any frame that never appears:\n",
    "tracks_per_frame_full = tracks_per_frame.reindex(full_frame_index, fill_value=0)\n",
    "\n",
    "\n",
    "# Count “long” tracks (mc_tid != 0) per frame in exactly the same way:\n",
    "unf_tracks_per_frame = (\n",
    "    tracks_df[tracks_df['mc_tid'] != 0]\n",
    "    .groupby('frameId')['mc_tid']\n",
    "    .nunique()\n",
    ")\n",
    "unf_tracks_per_frame_full = unf_tracks_per_frame.reindex(full_frame_index, fill_value=0)\n",
    "\n",
    "\n",
    "# Now build discrete histograms over {0,1,2,3,4} tracks/frame:\n",
    "#    Any frame with ≥5 tracks we lump into the “4” bin (so that our x‐axis is 0 through 4).\n",
    "\n",
    "# Count how many frames have exactly k six‐hit recurl‐tracks:\n",
    "tracks_counts = tracks_per_frame_full.value_counts().sort_index()\n",
    "# Similarly for “long” tracks:\n",
    "unf_tracks_counts = unf_tracks_per_frame_full.value_counts().sort_index()\n",
    "\n",
    "# We only want bins 0–4 (anything≥5 goes into index 4):\n",
    "desired_N = range(0, 5)\n",
    "tracks_counts = tracks_counts.reindex(desired_N, fill_value=0)\n",
    "unf_tracks_counts = unf_tracks_counts.reindex(desired_N, fill_value=0)\n",
    "\n",
    "\n",
    "# Define edges and centers for a step‐histogram over 0,1,2,3,4:\n",
    "bin_edges   = np.array([-0.5, 0.5, 1.5, 2.5, 3.5, 4.5])\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "\n",
    "# Poisson errors = sqrt(count):\n",
    "errors_tracks = np.sqrt(tracks_counts.values)\n",
    "errors_unf   = np.sqrt(unf_tracks_counts.values)\n",
    "\n",
    "# --- Plot for Six-Hit Recurls ---\n",
    "\n",
    "plt.figure(figsize=(3.6, 4))\n",
    "# Use plt.stairs to draw the step histogram.\n",
    "plt.stairs(tracks_counts.values, bin_edges, fill=False, color='green', linewidth=1.5, label='Six-Hit Recurls')\n",
    "# Overlay error bars.\n",
    "plt.errorbar(bin_centers, tracks_counts.values, yerr=errors_tracks, fmt='none', color='green', capsize=3)\n",
    "\n",
    "plt.xlabel('Number of Six-Hit \\n Tracks per Frame', fontsize=14)\n",
    "plt.ylabel('Number of Frames', fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=13)\n",
    "plt.grid(axis='both', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "plt.ylim(0, 31200)\n",
    "plt.tight_layout()\n",
    "plt.xticks(bin_centers, [str(i) for i in range(0, 5)])\n",
    "plt.show()\n",
    "\n",
    "# --- Plot for Long Tracks ---\n",
    "\n",
    "plt.figure(figsize=(3.6, 4))\n",
    "plt.stairs(unf_tracks_counts.values, bin_edges, fill=False, color='orange', linewidth=1.5, label='Long Tracks')\n",
    "plt.errorbar(bin_centers, unf_tracks_counts.values, yerr=errors_unf, fmt='none', color='orange', capsize=3)\n",
    "\n",
    "plt.xlabel('Number of Long \\n Tracks per Frame', fontsize=14)\n",
    "plt.ylabel('Number of Frames', fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=13)\n",
    "plt.grid(axis='both', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "plt.ylim(0, 16100)\n",
    "plt.tight_layout()\n",
    "plt.xticks(bin_centers, [str(i) for i in range(0, 5)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation from Hits (triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 'hits_df_unique'. this cell finds all hits from triplets, including or excluding noise (where mc_tid, mc_p, etc. = 0).\n",
    "# hits_df_unique has noise and is used for graph generation and plots.\n",
    "# hits_df_unique2 is clean from noise and is only used for plots. \n",
    "\n",
    "from flatten_hits import flatten_hits, dedupe_hits, apply_layer_assignment\n",
    "\n",
    "# Replace with the mu3e file you want eg. segs10_data, segsIC_data, segsbeam_data, segsmichel_data\n",
    "flat_hits_df     = flatten_hits(segsbeam_data) # here\n",
    "hits_df_unique   = dedupe_hits(flat_hits_df)\n",
    "hits_df_unique   = apply_layer_assignment(hits_df_unique)\n",
    "hits_df_unique2  = hits_df_unique[hits_df_unique.mc_tid!=0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total Hits After Flattening: {len(flat_hits_df)}\")\n",
    "print(\"\\nDeduplicated Hits DataFrame (hits_df_unique):\")\n",
    "print(hits_df_unique.head())\n",
    "print(f\"Total Unique Hits in hits_df_unique: {len(hits_df_unique)}\")\n",
    "print(f\"Total Unique Hits in hits_df_unique2: {len(hits_df_unique2)}\")\n",
    "\n",
    "# # i saved and loaded these as 'unique_hits_eee', 'unique_hits_eeevv', 'unique_hits_evv', 'unique_hits_beam'\n",
    "torch.save(hits_df_unique, 'unique_hits_beam.pt')\n",
    "# torch.save(hits_df_unique2, 'unique_hits2_eee.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlapping number of electrons seen per frame\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- load pre‐saved DataFrames without noise ---\n",
    "df_eee    = torch.load(\"unique_hits2_eee.pt\")\n",
    "df_eeevv  = torch.load(\"unique_hits2_eeevv.pt\")\n",
    "df_evv    = torch.load(\"unique_hits2_evv.pt\")\n",
    "df_beam   = torch.load(\"unique_hits2_beam.pt\")\n",
    "\n",
    "def get_multiplicity(df):\n",
    "    # compute unique‐mc_tid per frame\n",
    "    counts = df.groupby('frameId')['mc_tid'] \\\n",
    "               .nunique()\n",
    "    # reindex over the full observed frame‐ID range\n",
    "    fmin, fmax = counts.index.min(), counts.index.max()\n",
    "    counts = counts.reindex(range(fmin, fmax+1), fill_value=0)\n",
    "    # now build histogram of “how many frames saw k electrons”\n",
    "    freq = counts.value_counts().sort_index()\n",
    "    # reindex so we don't miss any k from 0…max\n",
    "    freq = freq.reindex(range(freq.index.max()+1), fill_value=0)\n",
    "    return freq\n",
    "\n",
    "mult_eee    = get_multiplicity(df_eee)\n",
    "mult_eeevv  = get_multiplicity(df_eeevv)\n",
    "mult_evv    = get_multiplicity(df_evv)\n",
    "mult_beam   = get_multiplicity(df_beam)\n",
    "\n",
    "# synchronize to common k‐axis\n",
    "max_k = max(m.index.max() for m in (mult_eee, mult_eeevv, mult_evv, mult_beam))\n",
    "mult_eee   = mult_eee.reindex(range(max_k+1), fill_value=0)\n",
    "mult_eeevv = mult_eeevv.reindex(range(max_k+1), fill_value=0)\n",
    "mult_evv   = mult_evv.reindex(range(max_k+1), fill_value=0)\n",
    "mult_beam  = mult_beam.reindex(range(max_k+1), fill_value=0)\n",
    "\n",
    "def normalize_and_extend(counts):\n",
    "    total = counts.sum()\n",
    "    norm  = counts.values / total\n",
    "    err   = np.sqrt(counts.values) / total\n",
    "    ext   = np.append(norm, norm[-1])  # so step plot closes\n",
    "    return norm, err, ext\n",
    "\n",
    "n_eee,   e_eee,   x_eee   = normalize_and_extend(mult_eee)\n",
    "n_eeevv, e_eeevv, x_eeevv = normalize_and_extend(mult_eeevv)\n",
    "n_evv,   e_evv,   x_evv   = normalize_and_extend(mult_evv)\n",
    "n_beam,  e_beam,  x_beam  = normalize_and_extend(mult_beam)\n",
    "\n",
    "# x‐axis\n",
    "x_edges   = np.arange(-0.5, max_k + 1.5, 1.0)\n",
    "x_centers = np.arange(0, max_k+1)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.step(x_edges, x_eee,    where='post', label='Signal', color='blue',   lw=1.5)\n",
    "plt.step(x_edges, x_eeevv,  where='post', label='I.C.',   color='orange', lw=1.5)\n",
    "plt.step(x_edges, x_evv,    where='post', label='Michel', color='red',    lw=1.5)\n",
    "plt.step(x_edges, x_beam,   where='post', label='Beam',   color='green',  lw=1.5)\n",
    "\n",
    "# error bars\n",
    "plt.errorbar(x_centers, n_eee,   yerr=e_eee,   fmt='none', ecolor='blue',   capsize=3)\n",
    "plt.errorbar(x_centers, n_eeevv, yerr=e_eeevv, fmt='none', ecolor='orange', capsize=3)\n",
    "plt.errorbar(x_centers, n_evv,   yerr=e_evv,   fmt='none', ecolor='red',    capsize=3)\n",
    "plt.errorbar(x_centers, n_beam,  yerr=e_beam,  fmt='none', ecolor='green',  capsize=3)\n",
    "\n",
    "plt.xlabel('Number of $e^-/e^+$ per Frame', fontsize=14)\n",
    "plt.ylabel('Frequency Density',             fontsize=14)\n",
    "# plt.yscale('log')\n",
    "plt.xticks(x_centers, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xlim(-0.5, 11 + 0.5)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of hits per frame overlapping\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pre-saved unique hits DataFrames.\n",
    "df_eee   = torch.load(\"unique_hits2_eee.pt\")\n",
    "df_eeevv = torch.load(\"unique_hits2_eeevv.pt\")\n",
    "df_evv   = torch.load(\"unique_hits2_evv.pt\")\n",
    "df_beam  = torch.load(\"unique_hits2_beam.pt\")\n",
    "\n",
    "# --- dynamically infer total frames from each DataFrame's frameId range ---\n",
    "def infer_total_frames(df):\n",
    "    mi, ma = int(df['frameId'].min()), int(df['frameId'].max())\n",
    "    return ma - mi + 1\n",
    "\n",
    "total_frames = {\n",
    "    \"Signal\":             infer_total_frames(df_eee),\n",
    "    \"Internal Conversion\":infer_total_frames(df_eeevv),\n",
    "    \"Michel\":             infer_total_frames(df_evv),\n",
    "    \"Beam\":               infer_total_frames(df_beam)\n",
    "}\n",
    "\n",
    "print(\"Inferred total frames per sample:\", total_frames)\n",
    "\n",
    "def get_hits_per_frame_with_zeros(df, total):\n",
    "    # start with an array of zeros for every frame\n",
    "    hits = np.zeros(total, dtype=int)\n",
    "    # count hits/frame in actual data\n",
    "    counts = df.groupby(\"frameId\").size()\n",
    "    # only fill in those frameIds which are within [0, total)\n",
    "    valid = counts.index[counts.index < total]\n",
    "    hits[valid] = counts.loc[valid].values\n",
    "    return hits\n",
    "\n",
    "# Compute arrays\n",
    "hits_eee   = get_hits_per_frame_with_zeros(df_eee,   total_frames[\"Signal\"])\n",
    "hits_eeevv = get_hits_per_frame_with_zeros(df_eeevv, total_frames[\"Internal Conversion\"])\n",
    "hits_evv   = get_hits_per_frame_with_zeros(df_evv,   total_frames[\"Michel\"])\n",
    "hits_beam  = get_hits_per_frame_with_zeros(df_beam,  total_frames[\"Beam\"])\n",
    "\n",
    "# choose binning (e.g. 0–32 in 16 bins → width=2)\n",
    "bins     = np.linspace(-0.5, 31.5, 17)\n",
    "centers  = 0.5 * (bins[:-1] + bins[1:])\n",
    "width    = bins[1] - bins[0]\n",
    "\n",
    "def compute_density_and_error(arr, total):\n",
    "    counts, _ = np.histogram(arr, bins=bins)\n",
    "    density   = counts / (total * width)\n",
    "    err       = np.sqrt(counts) / (total * width)\n",
    "    # extend for step-plot\n",
    "    return np.concatenate([density, [density[-1]]]), \\\n",
    "           np.concatenate([err,     [err[-1]]])\n",
    "\n",
    "den_eee,   err_eee   = compute_density_and_error(hits_eee,   total_frames[\"Signal\"])\n",
    "den_eeevv, err_eeevv = compute_density_and_error(hits_eeevv, total_frames[\"Internal Conversion\"])\n",
    "den_evv,   err_evv   = compute_density_and_error(hits_evv,   total_frames[\"Michel\"])\n",
    "den_beam,  err_beam  = compute_density_and_error(hits_beam,  total_frames[\"Beam\"])\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# step-plots\n",
    "plt.step(bins, den_eee,    where=\"post\", color=\"blue\",   lw=1.5, label=\"Signal\")\n",
    "plt.step(bins, den_eeevv,  where=\"post\", color=\"orange\", lw=1.5, label=\"I.C.\")\n",
    "plt.step(bins, den_evv,    where=\"post\", color=\"red\",    lw=1.5, label=\"Michel\")\n",
    "plt.step(bins, den_beam,   where=\"post\", color=\"green\",  lw=1.5, label=\"Beam\")\n",
    "\n",
    "# errorbars at bin centers\n",
    "plt.errorbar(centers, den_eee[:-1],   yerr=err_eee[:-1],   fmt=\"none\", ecolor=\"blue\",   capsize=3)\n",
    "plt.errorbar(centers, den_eeevv[:-1], yerr=err_eeevv[:-1], fmt=\"none\", ecolor=\"orange\", capsize=3)\n",
    "plt.errorbar(centers, den_evv[:-1],   yerr=err_evv[:-1],   fmt=\"none\", ecolor=\"red\",    capsize=3)\n",
    "plt.errorbar(centers, den_beam[:-1],  yerr=err_beam[:-1],  fmt=\"none\", ecolor=\"green\",  capsize=3)\n",
    "\n",
    "plt.xlabel(\"Number of Hits per Frame\", fontsize=14)\n",
    "plt.ylabel(\"Frequency Density\",      fontsize=14)\n",
    "plt.xlim(bins[0], bins[-1])\n",
    "plt.tick_params(labelsize=12)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.legend(fontsize=11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of graphs possible across all frames.\n",
    "# Assumes you already have hits_df_unique loaded\n",
    "# this can be run for any of the four datasets.\n",
    "\n",
    "hits_df_unique = torch.load('unique_hits_eee.pt') # 'unique_hits_eee', 'unique_hits_eeevv', 'unique_hits_evv', 'unique_hits_beam'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Loop over frames to count possible graphs per frame\n",
    "frames = hits_df_unique.groupby('frameId')\n",
    "\n",
    "total_graphs = 0\n",
    "frame_totals = []\n",
    "max_frame = None\n",
    "max_count = -1\n",
    "\n",
    "for frame_id, group in frames:\n",
    "    # count hits in each layer\n",
    "    c1  = len(group[group['layer'] == '1'])\n",
    "    c2  = len(group[group['layer'] == '2'])\n",
    "    c3  = len(group[group['layer'] == '3'])\n",
    "    c4  = len(group[group['layer'] == '4'])\n",
    "    c4p = len(group[group['layer'] == '4+'])\n",
    "    c3p = len(group[group['layer'] == '3+'])\n",
    "    c4m = len(group[group['layer'] == '4-'])\n",
    "    c3m = len(group[group['layer'] == '3-'])\n",
    "    \n",
    "    # two trajectory possibilities per frame\n",
    "    graphs_traj1 = c1 * c2 * c3 * c4  * c4p * c3p\n",
    "    graphs_traj2 = c1 * c2 * c3 * c4  * c4m * c3m\n",
    "    \n",
    "    frame_total = graphs_traj1 + graphs_traj2\n",
    "    frame_totals.append(frame_total)\n",
    "    total_graphs += frame_total\n",
    "    \n",
    "    if frame_total > max_count:\n",
    "        max_count = frame_total\n",
    "        max_frame = frame_id\n",
    "\n",
    "print(f\"Total possible graphs across all frames: {total_graphs}\")\n",
    "print(f\"Frame with most possible graphs: {max_frame} → {max_count} trajectories\")\n",
    "\n",
    "# 2) Compute number of completely empty frames dynamically\n",
    "unique_frames = hits_df_unique['frameId'].unique()\n",
    "min_f, max_f = unique_frames.min(), unique_frames.max()\n",
    "total_frames = max_f - min_f + 1\n",
    "n_seen = len(unique_frames)\n",
    "num_empty = total_frames - n_seen\n",
    "\n",
    "print(f\"Frame IDs range from {min_f} to {max_f} → {total_frames} total frames\")\n",
    "print(f\"{n_seen} frames with ≥1 graph, so {num_empty} empty frames\")\n",
    "\n",
    "# 3) Combine non-empty counts with zeros for empty frames\n",
    "all_totals = frame_totals + [0] * num_empty\n",
    "\n",
    "# 4) Plot histogram for full range (log y-axis)\n",
    "counts, bin_edges = np.histogram(all_totals, bins=35)\n",
    "bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "errors = np.sqrt(counts)\n",
    "\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.hist(all_totals, bins=35, histtype='step', linewidth=1.4, edgecolor='black')\n",
    "plt.errorbar(bin_centers, counts, yerr=errors,\n",
    "             fmt='none', ecolor='black', capsize=3)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Number of Possible Graphs per Frame', fontsize=14)\n",
    "plt.ylabel('Number of Frames', fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "# plt.ylim(1.1e-10,3.2e-5)\n",
    "ax = plt.gca()\n",
    "offset = ax.xaxis.get_offset_text()\n",
    "offset.set_position((1.1, 0))\n",
    "offset.set_ha('center')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# 5) Plot zoomed‐in histogram (0–50 graphs)\n",
    "counts_z, bin_edges_z = np.histogram(all_totals, bins=35, range=(0,50))\n",
    "bin_centers_z = 0.5 * (bin_edges_z[:-1] + bin_edges_z[1:])\n",
    "errors_z = np.sqrt(counts_z)\n",
    "\n",
    "plt.figure(figsize=(4.5, 3))\n",
    "plt.hist(all_totals, bins=35, range=(0,50), histtype='step', linewidth=1.4, edgecolor='black')\n",
    "plt.errorbar(bin_centers_z, counts_z, yerr=errors_z,\n",
    "             fmt='none', ecolor='black', capsize=3)\n",
    "plt.xlabel('Number of Possible Graphs per Frame', fontsize=14)\n",
    "plt.ylabel('Number of Frames', fontsize=14)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6hit only; All Frame Execution: Building and Validating Tracks\n",
    "from build6hittracks import build_and_validate_tracks\n",
    "import pandas as pd\n",
    "import torch \n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# change the hits for the correct file: unique_hits_eee, unique_hits_eeevv, unique_hits_evv, unique_hits_beam\n",
    "hits_df_unique   = torch.load('unique_hits_beam.pt') # here\n",
    "\n",
    "all_tracks = []\n",
    "for frame_id in tqdm(hits_df_unique['frameId'].unique()):\n",
    "    validated = build_and_validate_tracks(\n",
    "        hits_df_unique,\n",
    "        frame_id,\n",
    "        layer_sequences = [\n",
    "            ['1','2','3','4','4+','3+'],\n",
    "            ['1','2','3','4','4-','3-']\n",
    "        ],\n",
    "        center_tolerance=50, # mm\n",
    "        radius_tolerance=50, # mm \n",
    "        pitch_tolerance=40, # mm/rad\n",
    "        distance_constraints={'1-2':30,'3-4':60,'5-6':60}, # mm \n",
    "        z_distance_error_margin=0.65 \n",
    "        # look at the plots showing the true value for these constraints for 6hit graphs if you must adjust\n",
    "        # the bad constraint with arbitrary value 0.4 for zratio z12/z34 is found in build6hittracks.py; def build_and_validate_tracks\n",
    "    )\n",
    "    all_tracks.extend(validated)\n",
    "\n",
    "print(f\"\\nTotal Number of Validated Tracks: {len(all_tracks)}\")\n",
    "validated_tracks_df = pd.DataFrame(all_tracks)\n",
    "torch.save(validated_tracks_df, \"validated_tracks_beam_May31.pt\") # also change name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of LOG number of post-validation graphs per frame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "hits_df_unique = torch.load('unique_hits_eee.pt')\n",
    "validated_tracks_df = torch.load('validated_tracks_eee_May31.pt')\n",
    "# --- 1) Build per‐frame totals -------------------------------------\n",
    "unique_frames = hits_df_unique['frameId'].unique()\n",
    "min_f, max_f = unique_frames.min(), unique_frames.max()\n",
    "total_frames = max_f - min_f + 1\n",
    "frame_counts = (\n",
    "    validated_tracks_df\n",
    "      .groupby('frameId')\n",
    "      .size()\n",
    "      .reindex(np.arange(total_frames), fill_value=0)\n",
    "      .values\n",
    ")\n",
    "\n",
    "# --- 2) Compute histogram + Poisson errors -------------------------------\n",
    "counts, bin_edges = np.histogram(frame_counts, bins=35)\n",
    "bin_centers      = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "errors           = np.sqrt(counts)  # Poisson\n",
    "\n",
    "# --- 3) Full‐range (log‐y) counts plot ------------------------------------\n",
    "plt.figure(figsize=(4.5,3))\n",
    "plt.hist(frame_counts,\n",
    "         bins=35,\n",
    "         histtype='step',\n",
    "         edgecolor='navy',\n",
    "         linewidth=1.4)\n",
    "plt.errorbar(bin_centers, counts,\n",
    "             yerr=errors,\n",
    "             fmt='none',\n",
    "             ecolor='black',\n",
    "             capsize=3,\n",
    "             elinewidth=1)\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.xlabel('Number of Generated Graphs per Frame', fontsize=13)\n",
    "plt.ylabel('Counts',                  fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax     = plt.gca()\n",
    "offset = ax.xaxis.get_offset_text()\n",
    "offset.set_position((1.1, 0))     # tweak these numbers\n",
    "offset.set_ha('center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Real and Fakes based on Track IDs\n",
    "# looks up the mc/truth info from the hit ids. ensures all hits share the same non-zero track id in order to be labelled 'real'\n",
    "# real tracks have meaningful helical/momentum information, so this information is also saved.\n",
    "# i recently saw that i was accepting any track with matching mc_tids with any number of noise hits as real. this is fixed now;\n",
    "# it only resulted in a shift (real to fake) of a few hundred graphs out of the 110,000 saved.\n",
    "import torch; import pandas as pd\n",
    "from determinerealorfake import all_hits_zero_tid, determine_track_tid, determine_track_type, determine_track_pt, angle_from_map\n",
    "\n",
    "hits_df_unique        = torch.load(\"unique_hits_beam.pt\")\n",
    "validated_tracks_df   = torch.load(\"validated_tracks_beam_May31.pt\")\n",
    "\n",
    "# 2) Build lookup maps from hit_id → mc_tid, mc_type, and the three angles\n",
    "hit_to_tid_map  = hits_df_unique.set_index(\"hit_id\")[\"mc_tid\"].to_dict()\n",
    "hit_to_type_map = hits_df_unique.set_index(\"hit_id\")[\"mc_type\"].to_dict()\n",
    "hit_angle_map   = hits_df_unique.set_index(\"hit_id\")[[\"mc_phi\",\"mc_theta\",\"mc_lam\"]].to_dict(\"index\")\n",
    "\n",
    "mask_all_zero = validated_tracks_df[\"hits\"].apply(lambda hits: all_hits_zero_tid(hits, hit_to_tid_map))\n",
    "validated_tracks_df = validated_tracks_df[~mask_all_zero].copy()\n",
    "\n",
    "# Apply the function to assign mc_tid to each validated track\n",
    "validated_tracks_df[\"mc_tid\"]      = validated_tracks_df[\"hits\"].apply(lambda hits: determine_track_tid(hits, hit_to_tid_map))\n",
    "validated_tracks_df[\"mc_type\"]     = validated_tracks_df[\"hits\"].apply(lambda hits: determine_track_type(hits, hit_to_type_map))\n",
    "validated_tracks_df[\"is_real_track\"] = validated_tracks_df[\"mc_tid\"].apply(lambda x: isinstance(x, int) and x != 0)\n",
    "\n",
    "real_tracks_df = validated_tracks_df[validated_tracks_df['is_real_track']].copy().reset_index(drop=True)\n",
    "fake_tracks_df = validated_tracks_df[~validated_tracks_df['is_real_track']].copy().reset_index(drop=True)\n",
    "\n",
    "# Display summaries\n",
    "print(f\"Total Real Tracks: {len(real_tracks_df)}\")\n",
    "print(f\"Total Fake Tracks: {len(fake_tracks_df)}\")\n",
    "\n",
    "real_tracks_df[[\"mc_pt\",\"mc_p\"]] = (real_tracks_df[\"hits\"].apply(lambda hits: determine_track_pt(hits)).apply(pd.Series))\n",
    "for angle in (\"mc_phi\",\"mc_theta\",\"mc_lam\"):\n",
    "    real_tracks_df[angle] = real_tracks_df[\"hits\"].apply(\n",
    "        lambda hits: angle_from_map(hits, angle, hit_angle_map))\n",
    "\n",
    "# Identify frames with the most fakes\n",
    "most_fakes_per_frame = fake_tracks_df['frameId'].value_counts()\n",
    "print(\"\\nFrames with Most Fake Tracks:\")\n",
    "print(most_fakes_per_frame.head())\n",
    "\n",
    "# Total Real Tracks. eee_May31: 31370. eeevv_May31: 13066. evv_May31: 11884. beam_May31: 5912\n",
    "# Total Fake Tracks. eee_May31: 87082. eeevv_May31: 48974. evv_May31: 58   . beam_May31: 364874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(real_tracks_df.head())\n",
    "# print(fake_tracks_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code finds the graph (global) feature and edge feature means and stdevs for normalisation.\n",
    "# node features (hit positions and onehot layer ids) are not normalised.\n",
    "# this only needs to be run once with the signal real tracks loaded as 'real_tracks_df_temp' \n",
    "# as future normalisation is with respect to signal in the following cell.\n",
    "# perhaps this could use sixhit_tracks_df instead of real_tracks_df.\n",
    "from graphgeneration import extract_geom_features_from_df6, compute_global_edge_features_stats6\n",
    "import torch\n",
    "real_tracks_df_temp = torch.load('real_tracks_eee_May31.pt')\n",
    "# A) Compute and save geometric‐feature normalization stats\n",
    "geom_array = extract_geom_features_from_df6(real_tracks_df_temp)   # shape = (N_real, 10)\n",
    "geom_means = geom_array.mean(axis=0)\n",
    "geom_stds  = geom_array.std(axis=0)\n",
    "\n",
    "graph_norm_stats = {\n",
    "    'geom_means': geom_means,\n",
    "    'geom_stds':  geom_stds\n",
    "}\n",
    "torch.save(graph_norm_stats, \"graph_norm_stats.pt\")\n",
    "\n",
    "# B) Compute and save edge‐feature normalization stats\n",
    "#    First convert DataFrame → list of track‐dicts\n",
    "tracks_list6 = real_tracks_df_temp.to_dict(\"records\")\n",
    "edge_stats   = compute_global_edge_features_stats6(tracks_list6)\n",
    "\n",
    "edge_norm_stats = {\n",
    "    'global_distance_means': edge_stats['distance'][0],\n",
    "    'global_distance_stds':  edge_stats['distance'][1],\n",
    "    'global_lambda_means':   edge_stats['lambda'][0],\n",
    "    'global_lambda_stds':    edge_stats['lambda'][1],\n",
    "    'global_t_means':        edge_stats['tdist'][0],\n",
    "    'global_t_stds':         edge_stats['tdist'][1],\n",
    "    'global_z_means':        edge_stats['zdist'][0],\n",
    "    'global_z_stds':         edge_stats['zdist'][1]\n",
    "}\n",
    "torch.save(edge_norm_stats, \"edge_norm_stats.pt\")\n",
    "print(\"Saved graph_norm_stats.pt and edge_norm_stats.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6hit; track to graph\n",
    "# label 0: positron, 1: electron, 2: fake.\n",
    "# beam takes me 5min to save\n",
    "# make sure real and fake_tracks_df are loaded with the correct set\n",
    "# and change saved file name at the bottom of this cell.\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from graphgeneration import (layer_map, extract_constraints6, extract_geom_features_from_df6, compute_global_edge_features_stats6, track_to_graph6)\n",
    "\n",
    "graph_norm_stats = torch.load(\"graph_norm_stats.pt\")\n",
    "geom_means = graph_norm_stats[\"geom_means\"]\n",
    "geom_stds  = graph_norm_stats[\"geom_stds\"]\n",
    "\n",
    "edge_norm_stats = torch.load(\"edge_norm_stats.pt\")\n",
    "global_distance_means = edge_norm_stats[\"global_distance_means\"]\n",
    "global_distance_stds  = edge_norm_stats[\"global_distance_stds\"]\n",
    "global_lambda_means   = edge_norm_stats[\"global_lambda_means\"]\n",
    "global_lambda_stds    = edge_norm_stats[\"global_lambda_stds\"]\n",
    "global_t_means        = edge_norm_stats[\"global_t_means\"]\n",
    "global_t_stds         = edge_norm_stats[\"global_t_stds\"]\n",
    "global_z_means        = edge_norm_stats[\"global_z_means\"]\n",
    "global_z_stds         = edge_norm_stats[\"global_z_stds\"]\n",
    "\n",
    "graphs6 = []\n",
    "\n",
    "# (a) if you want to apply class weighting. i found that it performed well without but perhaps there are other methods\n",
    "real_tracks_df[\"mc_pid\"] = real_tracks_df[\"hits\"].apply(\n",
    "    lambda hits: hits[0][\"mc_pid\"] if (isinstance(hits, (list, tuple)) and len(hits) > 0) else None\n",
    ")\n",
    "num_elec  = len(real_tracks_df[ real_tracks_df[\"mc_pid\"] ==  11 ])\n",
    "num_pos  = len(real_tracks_df[ real_tracks_df[\"mc_pid\"] == -11 ])\n",
    "total_real= num_elec + num_pos\n",
    "print(\"total_real_tracks\", total_real, \"num_electrons\", num_elec, \"num_positrons\", num_pos)\n",
    "\n",
    "# (b) Build graphs for real‐tracks (label 0=e+, 1=e-)\n",
    "for track in real_tracks_df.to_dict(\"records\"):\n",
    "    mc_pid = track.get(\"mc_pid\", None)\n",
    "    if mc_pid == -11:\n",
    "        label = 0  # positron\n",
    "    elif mc_pid == 11:\n",
    "        label = 1  # electron\n",
    "    else:\n",
    "        # skip anything else (shouldn't happen)\n",
    "        continue\n",
    "\n",
    "    data = track_to_graph6(\n",
    "        track, label, layer_map, geom_means, geom_stds, \n",
    "        global_distance_means, global_distance_stds,\n",
    "        global_lambda_means, global_lambda_stds, global_t_means,\n",
    "        global_t_stds, global_z_means, global_z_stds\n",
    "    )\n",
    "    # i attached a uniform weight as a placeholder but class weighting can be \n",
    "    # applied in the GNN notebook if you want to do that instead\n",
    "    data.charge_weight = 1.0\n",
    "    graphs6.append(data)\n",
    "\n",
    "# (c) Build graphs for fake‐tracks (label = 2)\n",
    "for track in fake_tracks_df.to_dict(\"records\"):\n",
    "    data = track_to_graph6(\n",
    "        track, 2,  # label=2 means “fake” \n",
    "        layer_map, geom_means,\n",
    "        geom_stds, global_distance_means, global_distance_stds,\n",
    "        global_lambda_means, global_lambda_stds, global_t_means,\n",
    "        global_t_stds, global_z_means, global_z_stds\n",
    "    )\n",
    "    data.charge_weight = 1.0\n",
    "    graphs6.append(data)\n",
    "\n",
    "print(f\"Created {len(graphs6)} graphs.\")\n",
    "torch.save(graphs6, 'Graphs_beam_May31.pt') # change name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect a graph\n",
    "# graphs6 = torch.load('Graphs_eee_May31.pt') # this is slow\n",
    "print(f\"Loaded {len(graphs6)} graphs.\")\n",
    "\n",
    "first_graph = graphs6[0]\n",
    "print(\"Frame ID:\")\n",
    "print(first_graph.frameId)\n",
    "print(\"First graph details:\")\n",
    "print(first_graph)\n",
    "print(\"Attributes:\", first_graph.keys())\n",
    "\n",
    "print(\"Node features (x):\")\n",
    "print(first_graph.x)\n",
    "print(\"Edge index:\")\n",
    "print(first_graph.edge_index)\n",
    "print(\"Label:\")\n",
    "print(first_graph.label)\n",
    "print(\"Graph attributes (raw and normalized parameters):\")\n",
    "print(first_graph.graph_attr)\n",
    "print(first_graph.raw_graph_attr)\n",
    "print(\"Edge attributes:\")\n",
    "print(first_graph.edge_attr)\n",
    "print(\"Hit Positions:\")\n",
    "print(first_graph.hit_pos)\n",
    "print('mc_pt =', first_graph.mc_pt)\n",
    "print('mc_p =', first_graph.mc_p)\n",
    "print('mc_phi =', first_graph.mc_phi)\n",
    "print('mc_lam =', first_graph.mc_lam)\n",
    "print('mc_theta =', first_graph.mc_theta)\n",
    "print('mc_type =', first_graph.mc_type)\n",
    "print('mc_tid =', first_graph.mc_tid)\n",
    "print(\" extra features required\", first_graph.graph_attr.shape) # should be 10 ie. (torch.Size([1, 10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of different tids used to generate graphs\n",
    "def count_unique_tids(mc_tid):\n",
    "    if isinstance(mc_tid, list):\n",
    "        return len(set(mc_tid))\n",
    "    elif isinstance(mc_tid, int):\n",
    "        return 1 if mc_tid != 0 else 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "validated_tracks_df['num_unique_tids'] = validated_tracks_df['mc_tid'].apply(count_unique_tids)\n",
    "\n",
    "print(validated_tracks_df['num_unique_tids'].value_counts().sort_index())\n",
    "\n",
    "tid_counts = validated_tracks_df['num_unique_tids'].value_counts().sort_index().reset_index()\n",
    "tid_counts.columns = ['num_unique_tids', 'count']\n",
    "tid_counts = tid_counts[tid_counts['num_unique_tids'] > 0]\n",
    "\n",
    "print(\"\\nDistribution of Unique TIDs per Track:\")\n",
    "print(tid_counts)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "ax = sns.barplot(\n",
    "    x='num_unique_tids',\n",
    "    y='count',\n",
    "    data=tid_counts,\n",
    "    palette='magma'\n",
    ")\n",
    "\n",
    "colours = ['green' if n == 1 else 'red' for n in tid_counts['num_unique_tids']]\n",
    "for patch, col in zip(ax.patches, colours):\n",
    "    patch.set_facecolor(col)\n",
    "    height = patch.get_height()\n",
    "    ax.text(\n",
    "        patch.get_x() + patch.get_width() / 2,\n",
    "        height + (tid_counts['count'].max() * 0.01),\n",
    "        f\"{int(height):,}\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=12,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.xlabel('N Unique TIDs per Track', fontsize=14)\n",
    "plt.ylabel('N Validated Graphs', fontsize=14)\n",
    "plt.xticks(ticks=range(len(tid_counts)), labels=tid_counts['num_unique_tids'], rotation=0, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.ylim(0, tid_counts['count'].max() * 1.1)\n",
    "plt.grid(axis='y', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "plt.tick_params(axis='both', labelsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the missing real tracks (lost through constraints)\n",
    "found_ids = set(real_tracks_df['mc_tid'])\n",
    "truth_ids = set(sixhit_tracks_df['mc_tid'])\n",
    "missing_ids = truth_ids - found_ids\n",
    "print(f\"Total truth tracks: {len(truth_ids)}\")\n",
    "print(f\"Total found by constraints: {len(found_ids)}\")\n",
    "print(f\"Tracks missing: {len(missing_ids)}\")\n",
    "\n",
    "missing_real_tracks_df = sixhit_tracks_df[\n",
    "    sixhit_tracks_df['mc_tid'].isin(missing_ids)\n",
    "].drop_duplicates(subset=['mc_tid', 'frameId']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a frame for the generated graphs (and missing real tracks)!\n",
    "from plotlyhelper import visualise_frame1, wireframe_traces\n",
    "# in frame 474 for signal (eee), you will see an example of two very close hits in layer 1\n",
    "# one of these hits has mc_tid=0 (noise) and is used for making fakes.\n",
    "# in this case, the noise hit is not due to the overlapping ladders,\n",
    "# for overlapping double hits, you'd expect the electron path to be a fixed\n",
    "# radius from the trajectory's bending centre. so this is correct.\n",
    "# in the same frame however, it appears that the real track is missed\n",
    "\n",
    "target_frame_id = 474  # Replace with frame ID\n",
    "\n",
    "# Call the visualization function\n",
    "visualise_frame1(\n",
    "    frame_id=target_frame_id,\n",
    "    hits_df_unique=hits_df_unique,               # Use hits_df_unique here\n",
    "    real_tracks_df=real_tracks_df,               # Pass real_tracks_df\n",
    "    fake_tracks_df=fake_tracks_df,               # Pass fake_tracks_df\n",
    "    missing_real_tracks_df=missing_real_tracks_df,  # Pass missing_real_tracks here\n",
    "    wireframe_traces=wireframe_traces            # Wireframe traces as before\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'real_tracks_df' and 'fake_tracks_df' are already defined\n",
    "\n",
    "# Function to explode list columns into separate rows\n",
    "def explode_list_column(df, column_name):\n",
    "    return df.explode(column_name).reset_index(drop=True)\n",
    "\n",
    "# Explode 'pitches' and 'radii' for both real and fake tracks\n",
    "real_pitches = explode_list_column(real_tracks_df, 'pitches')['pitches']\n",
    "fake_pitches = explode_list_column(fake_tracks_df, 'pitches')['pitches']\n",
    "\n",
    "real_radii = explode_list_column(real_tracks_df, 'radii')['radii']\n",
    "fake_radii = explode_list_column(fake_tracks_df, 'radii')['radii']\n",
    "\n",
    "# real_angle_pT = explode_list_column(real_tracks_df, 'angle_diffs')['angle_diffs']\n",
    "# fake_angle_pT = explode_list_column(fake_tracks_df, 'angle_diffs')['angle_diffs']\n",
    "\n",
    "# Create DataFrames for plotting\n",
    "plot_data = pd.DataFrame({\n",
    "    'Pitch (mm/rad)': pd.concat([real_pitches, fake_pitches], ignore_index=True),\n",
    "    'Radius (mm)': pd.concat([real_radii, fake_radii], ignore_index=True),\n",
    "    'Track Type': ['Real'] * len(real_pitches) + ['Fake'] * len(fake_pitches)\n",
    "})\n",
    "\n",
    "# Separate the real and fake data for pitches\n",
    "real_pitches_data = plot_data[plot_data['Track Type'] == 'Real']['Pitch (mm/rad)']\n",
    "fake_pitches_data = plot_data[plot_data['Track Type'] == 'Fake']['Pitch (mm/rad)']\n",
    "\n",
    "# Define bin edges for pitches\n",
    "pitch_bins = np.linspace(-250, 250, 51)\n",
    "\n",
    "# Compute histograms for pitches\n",
    "real_pitches_counts, pitch_edges = np.histogram(real_pitches_data, bins=pitch_bins, density=True)\n",
    "fake_pitches_counts, _ = np.histogram(fake_pitches_data, bins=pitch_bins, density=True)\n",
    "\n",
    "# Plot real pitches as a filled histogram with blue opacity and outer edges outlined\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(real_pitches_data, bins=pitch_edges, density=True, color='green', alpha=0.3, histtype='stepfilled', label='Real')\n",
    "plt.step(pitch_edges[:-1], real_pitches_counts, where='post', color='green', linewidth=1.5)\n",
    "\n",
    "# Plot fake pitches as red triangles with black edges\n",
    "pitch_centers = 0.5 * (pitch_edges[:-1] + pitch_edges[1:])\n",
    "plt.scatter(pitch_centers, fake_pitches_counts, color='red', edgecolor='black', marker='^', label='Fake')\n",
    "\n",
    "plt.tick_params(axis='both', labelsize=14)\n",
    "plt.xlabel('Pitch (mm/rad)', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlim(-250, 250)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Separate the real and fake data for radius\n",
    "real_radius_data = plot_data[plot_data['Track Type'] == 'Real']['Radius (mm)']\n",
    "fake_radius_data = plot_data[plot_data['Track Type'] == 'Fake']['Radius (mm)']\n",
    "\n",
    "# Define bin edges for radius\n",
    "radius_bins = np.linspace(0, 300, 31)\n",
    "\n",
    "# Compute histograms for radius\n",
    "real_radius_counts, radius_edges = np.histogram(real_radius_data, bins=radius_bins, density=True)\n",
    "fake_radius_counts, _ = np.histogram(fake_radius_data, bins=radius_bins, density=True)\n",
    "\n",
    "# Plot real radius as a filled histogram with blue opacity and outer edges outlined\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(real_radius_data, bins=radius_edges, density=True, color='green', alpha=0.3, histtype='stepfilled', label='Real')\n",
    "plt.step(radius_edges[:-1], real_radius_counts, where='post', color='green', linewidth=1.5)\n",
    "\n",
    "# Plot fake radius as red triangles with black edges\n",
    "radius_centers = 0.5 * (radius_edges[:-1] + radius_edges[1:])\n",
    "plt.scatter(radius_centers, fake_radius_counts, color='red', edgecolor='black', marker='^', label='Fake')\n",
    "\n",
    "plt.tick_params(axis='both', labelsize=14)\n",
    "plt.xlabel('Bending Radius (mm)', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlim(0, 300)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_constraints(track):\n",
    "    hits = track['hits']\n",
    "    radial_12 = np.sqrt((hits[0]['x'] - hits[1]['x'])**2 + (hits[0]['y'] - hits[1]['y'])**2)\n",
    "    radial_34 = np.sqrt((hits[2]['x'] - hits[3]['x'])**2 + (hits[2]['y'] - hits[3]['y'])**2)\n",
    "    radial_56 = np.sqrt((hits[4]['x'] - hits[5]['x'])**2 + (hits[4]['y'] - hits[5]['y'])**2)\n",
    "    z_diff_12 = hits[0]['z'] - hits[1]['z']\n",
    "    z_diff_34 = hits[2]['z'] - hits[3]['z']\n",
    "    z_diff_56 = hits[4]['z'] - hits[5]['z']\n",
    "    return pd.Series({\n",
    "        'radial_12': radial_12, 'radial_34': radial_34, 'radial_56': radial_56,\n",
    "        'z_diff_12': z_diff_12, 'z_diff_34': z_diff_34, 'z_diff_56': z_diff_56\n",
    "    })\n",
    "\n",
    "# Extract constraint values for all tracks and merge with the DataFrame\n",
    "constraints_df = validated_tracks_df.apply(extract_constraints, axis=1)\n",
    "validated_tracks_df = pd.concat([validated_tracks_df, constraints_df], axis=1)\n",
    "\n",
    "# Separate real and fake tracks\n",
    "real_tracks_df = validated_tracks_df[validated_tracks_df['is_real_track']]\n",
    "fake_tracks_df = validated_tracks_df[~validated_tracks_df['is_real_track']]\n",
    "\n",
    "def melt_constraints(df, keys):\n",
    "    return df[keys].melt(var_name='constraint', value_name='value')\n",
    "\n",
    "radial_keys = ['radial_12', 'radial_34', 'radial_56']\n",
    "z_keys = ['z_diff_34', 'z_diff_56']\n",
    "\n",
    "real_radial = melt_constraints(real_tracks_df, radial_keys)\n",
    "fake_radial = melt_constraints(fake_tracks_df, radial_keys)\n",
    "real_z = melt_constraints(real_tracks_df, z_keys)\n",
    "fake_z = melt_constraints(fake_tracks_df, z_keys)\n",
    "\n",
    "# Define bins for radial distances and z differences\n",
    "radial_bins = np.linspace(0, 30, 31)  # 30 bins from 0 to 30 mm\n",
    "z_bins = np.linspace(-55, 55, 31)       # 30 bins from -55 to 55 mm\n",
    "\n",
    "# Extract the data (assumes real_radial and fake_radial dataframes exist as per previous cell)\n",
    "real_radial_data = real_radial['value']\n",
    "fake_radial_data = fake_radial['value']\n",
    "\n",
    "# Compute histograms for radial distances\n",
    "real_radial_counts, _ = np.histogram(real_radial_data, bins=radial_bins, density=True)\n",
    "fake_radial_counts, _ = np.histogram(fake_radial_data, bins=radial_bins, density=True)\n",
    "radial_centers = 0.5 * (radial_bins[:-1] + radial_bins[1:])\n",
    "\n",
    "# Plot Radial Distances Histogram\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(real_radial_data, bins=radial_bins, density=True, color='green', alpha=0.3,\n",
    "         histtype='stepfilled', label='Real')\n",
    "plt.step(radial_bins[:-1], real_radial_counts, where='post', color='green', linewidth=1.5)\n",
    "plt.scatter(radial_centers, fake_radial_counts, color='red', edgecolor='black', marker='^', label='Fake')\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.xlabel('Radial Distance between Nodes (mm)', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlim(0, 30)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Extract the data for z differences (assumes real_z and fake_z dataframes exist)\n",
    "real_z_data = real_z['value']\n",
    "fake_z_data = fake_z['value']\n",
    "\n",
    "# Compute histograms for z differences\n",
    "real_z_counts, _ = np.histogram(real_z_data, bins=z_bins, density=True)\n",
    "fake_z_counts, _ = np.histogram(fake_z_data, bins=z_bins, density=True)\n",
    "z_centers = 0.5 * (z_bins[:-1] + z_bins[1:])\n",
    "\n",
    "# Plot Z Differences Histogram\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(real_z_data, bins=z_bins, density=True, color='green', alpha=0.3,\n",
    "         histtype='stepfilled', label='Real')\n",
    "plt.step(z_bins[:-1], real_z_counts, where='post', color='green', linewidth=1.5)\n",
    "plt.scatter(z_centers, fake_z_counts, color='red', edgecolor='black', marker='^', label='Fake')\n",
    "plt.tick_params(axis='both', labelsize=12)\n",
    "plt.xlabel('Change in z between Nodes (mm)', fontsize=14)\n",
    "plt.ylabel('Density', fontsize=14)\n",
    "plt.xlim(-55, 55)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
